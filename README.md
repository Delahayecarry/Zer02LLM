# Zero2LLM

<div align="center">
    <img src="image/logo.jpg" alt="Zero2LLM Logo" width="250" height="250"/>
    <h3>从零开始学习大语言模型 | Learn LLM from Scratch</h3>
</div>


## 📚 项目简介

Zero2LLM 是一个专门面向初学者的大语言模型（LLM）学习项目。本项目采用循序渐进的方式，帮助你从最基础的 Attention 机制开始，一步步掌握现代大语言模型的核心概念和实现。

## 🎯 学习路线

我们建议按照以下顺序进行学习：

1. **基础 Attention 机制**
   - 理解 Attention 的基本原理
   - 实现简单的 Self-Attention
   - 掌握多头注意力机制
2. **NanoGPT 实现**
   - GPT 模型的基础架构
   - Transformer 解码器的实现(Decoder-only)
   - 预训练和微调过程
3. **MoE (Mixture of Experts) 模型**
   - 专家混合系统的原理
   - 动态路由机制
   - 可扩展性设计
4. **MLA (Multi Latent Attention) 模型**
   - Deepseek MLA
   - 实际应用案例
5. **Zero2LLM_all 综合实践**
   - 完整模型实现（MQA/MoE,RoPE..)
   - 性能优化
   - 实际部署经验

## 🚀 快速开始

```bash
# 克隆项目
git clone https://github.com/Delahayecarry/Zero2LLM.git

# 进入项目目录
cd Zero2LLM

# 开始学习
```

## 📂 项目结构

```
Zero2LLM/
├── Attention/        # 基础 Attention 实现
├── NanoGPT/         # NanoGPT 实现
├── moe/             # Mixture of Experts 实现
├── mla/             # Multi Latent Attention 实现
└── zero2llm_all/    # 完整项目实现（MQA/MoE,RoPE..)
```

## 📖 学习资源

- 每个模块都配备详细的说明文档
- 包含完整的代码注释
- 提供实践练习和示例
- 补充阅读材料和参考资源

## 🤝 贡献指南

我们欢迎所有形式的贡献，包括但不限于：

- 提交 Bug 报告
- 改进文档
- 提供新的示例
- 优化代码实现

## 📝 许可证

本项目采用 MIT 许可证 - 详见 [LICENSE](LICENSE) 文件

## ✨ 致谢

感谢所有为本项目做出贡献的开发者和研究者。

⭐️ 如果这个项目对你有帮助，欢迎点击 Star 支持！ 

