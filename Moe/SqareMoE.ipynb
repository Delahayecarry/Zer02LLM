{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - SqarseMoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Expert, self).__init__()\n",
    "        \n",
    "        self.expert = nn.Linear(input_dim, output_dim)\n",
    "        # 普通线性层\n",
    "        # 输入形状为(batch_size, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.expert(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 128])\n",
      "torch.Size([4, 4, 128])\n",
      "torch.Size([4, 128])\n"
     ]
    }
   ],
   "source": [
    "class BasicMoE(nn.Module):\n",
    "    def __init__ (self, num_experts, input_dim, output_dim):\n",
    "        super(BasicMoE, self).__init__()\n",
    "        self.experts = nn.ModuleList(\n",
    "            [nn.Linear(input_dim, output_dim) for _ in range(num_experts)]\n",
    "        )\n",
    "\n",
    "        self.gate = nn.Linear(input_dim, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x 的形状是 (batch_size, input_dim)\n",
    "        expert_weights = self.gate(x)\n",
    "        # expert_weights 的形状是 (batch_size, num_experts)\n",
    "        expert_outputs_list = [\n",
    "            experts(x).unsqueeze(1) for experts in self.experts\n",
    "            \n",
    "        ] # expert_outputs_list 是一个列表，列表中的每个元素是一个形状为 (batch_size, 1, output_dim) 的张量\n",
    "        print(expert_outputs_list[0].shape)\n",
    "        expert_outputs = torch.cat(expert_outputs_list, dim=1)\n",
    "        #  expert_outputs 的形状是 (batch_size, num_experts, output_dim)\n",
    "        print(expert_outputs.shape)\n",
    "\n",
    "  \n",
    "        # 所以我们可以使用 softmax 函数将其转换为概率分布\n",
    "        gate_outputs = F.softmax(expert_weights, dim=1)\n",
    "        # gate_outputs 的形状是 (batch_size, num_experts)\n",
    "        # 我们可以使用这些概率来加权每个专家的输出\n",
    "        weighted_expert_outputs = gate_outputs @ expert_outputs #执行的是严格矩阵乘法\n",
    "        # weighted_expert_outputs 的形状是 (batch_size, experts ,output_dim)\n",
    "        outputs = torch.sum(weighted_expert_outputs, dim=1)\n",
    "        # outputs 的形状是 (batch_size, output_dim) \n",
    "        # sum是对第二维度求和将experts维度消除\n",
    "        # print(weighted_expert_outputs.shape)\n",
    "        return outputs\n",
    "def test():\n",
    "    x = torch.randn(4, 512)\n",
    "    model = BasicMoE(4, 512, 128)\n",
    "    output = model(x)\n",
    "    print(output.shape)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16]) torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "class MoEconfig:\n",
    "    def __init__(self, top_k, expert_num, hidden_dim, share_expert_num=2):\n",
    "        self.top_k = top_k # 选择出的K个专家\n",
    "        self.expert_num = expert_num # 专家数量\n",
    "        self.share_expert_num = share_expert_num # 共享专家数量\n",
    "        self.hidden_dim = hidden_dim # 专家的隐藏层维度\n",
    "\n",
    "class MoErouter(nn.Module):\n",
    "    def __init__(self, hidden_num, expert_num, top_k):\n",
    "        super(MoErouter, self).__init__()\n",
    "        self.gate = nn.Linear(hidden_num, expert_num)\n",
    "        self.expert_num = expert_num\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        # hidden_state.shape=（batch_size*seq_len, hidden_dim）\n",
    "        # 计算路由\n",
    "        router_logits = self.gate(hidden_state) # gate是一个线性层，输入是hidden_state，输出是router_logits\n",
    "        # router_logits.shape=（batch_size*seq_len, expert_num）\n",
    "\n",
    "        # 计算专家进行softmax的概率\n",
    "        router_probs = F.softmax(router_logits, dim=-1)\n",
    "\n",
    "        # 计算topk专家的输出\n",
    "        router_weights, selected_experts = torch.topk(router_probs, self.top_k, dim=-1)\n",
    "        # router_weights.shape=（batch_size*seq_len, top_k）\n",
    "        # selected_experts.shape=（batch_size*seq_len, top_k）\n",
    "\n",
    "        # 进行专家权重的归一化\n",
    "        router_weights = router_weights / torch.sum(router_weights, dim=-1, keepdim=True)\n",
    "        router_weights = router_weights.to(hidden_state.dtype)\n",
    "\n",
    "        # 生成专家的mask\n",
    "        experts_mask = F.one_hot(selected_experts, num_classes=self.expert_num)\n",
    "        # experts_mask.shape=（batch_size*seq_len, top_k, expert_num）\n",
    "\n",
    "        # 将专家的mask转置\n",
    "        experts_mask = experts_mask.permute(2, 1, 0) \n",
    "        # permute函数用于维度的转换,参数是新的各个维度的索引\n",
    "        # 我希望它变成experts_mask.shape=（expert_num, top_k, batch_size*seq_len）\n",
    "\n",
    "        return router_logits, router_weights, selected_experts, experts_mask\n",
    "                # 返回的router_logits是路由的logits，\n",
    "                # router_weights是路由的概率，\n",
    "                # selected_experts是选择的专家，\n",
    "                # experts_mask是专家的mask\n",
    "\n",
    "        \n",
    "        \n",
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(SparseMoE, self).__init__()\n",
    "        self.expert_num = config.expert_num\n",
    "        self.share_expert_num = config.share_expert_num\n",
    "        self.hidden_dim = config.hidden_dim\n",
    "        self.experts = nn.ModuleList(\n",
    "            [\n",
    "                Expert(self.hidden_dim, self.hidden_dim) for _ in range(self.expert_num)  # 专家数量\n",
    "            ]\n",
    "        )\n",
    "        self.router = MoErouter(self.hidden_dim, self.expert_num, config.top_k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, hidden_dim = x.size()\n",
    "        hidden_state = x.view(-1, hidden_dim)  # 展开成(batch_size*seq_len, hidden_dim)\n",
    "        router_logits, router_weights, selected_experts_idx, expert_mask = self.router(hidden_state)\n",
    "        \n",
    "        final_hidden_states = torch.zeros((batch_size * seq_len, hidden_dim), dtype=torch.float32, device=x.device)\n",
    "\n",
    "        for experts_idx in range(self.expert_num):\n",
    "            expert_layer = self.experts[experts_idx]\n",
    "            idx, top_x = torch.where(expert_mask[experts_idx])  # 获取选中的专家及其位置\n",
    "            current_state = hidden_state.unsqueeze(0)[:, top_x, :].reshape(-1, hidden_dim)\n",
    "            current_hidden_state = expert_layer(current_state) * router_weights[top_x, idx].unsqueeze(-1)\n",
    "            final_hidden_states = final_hidden_states.index_add_(0, top_x, current_hidden_state.to(hidden_state.dtype))\n",
    "\n",
    "        final_hidden_states = final_hidden_states.view(batch_size, seq_len, hidden_dim)\n",
    "        return final_hidden_states, router_logits\n",
    "\n",
    "\n",
    "def test_token_level_moe():\n",
    "    x = torch.rand(2, 4, 16)\n",
    "    config = MoEconfig(2, 2, 16)\n",
    "    token_level_moe = SparseMoE(config)\n",
    "    out = token_level_moe(x)\n",
    "    print(out[0].shape, out[1].shape)\n",
    "\n",
    "test_token_level_moe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepSeekMoE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16]) torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class DeepSeekMoE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(DeepSeekMoE, self).__init__()\n",
    "        self.DeepSeekMoEconfig = SparseMoE(config)\n",
    "        self.shared_experts = nn.ModuleList(\n",
    "            [\n",
    "                Expert(config.hidden_dim, config.hidden_dim) for _ in range(config.share_expert_num)  # 专家数量\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 先过SparseMoE\n",
    "        sparse_moe_out, router_logits = self.DeepSeekMoEconfig(x)\n",
    "        # sparse_moe_out.shape=（batch_size, seq_len, hidden_dim）\n",
    "        # router_logits.shape=（batch_size*seq_len, expert_num）\n",
    "\n",
    "        # 过共享专家\n",
    "        shared_expert_out = [\n",
    "            expert(x) for expert in self.shared_experts\n",
    "        ] # shared_expert_out是一个列表，列表中的每个元素是一个tensor,\n",
    "        # 其中每个expert的shape是（batch_size, seq_len, hidden_dim）\n",
    "\n",
    "        # 拼接\n",
    "        shared_expert_out = torch.stack(shared_expert_out, dim=0).sum(dim=0)\n",
    "        # shared_expert_out.shape=（ batch_size, seq_len, hidden_dim）\n",
    "\n",
    "        return sparse_moe_out + shared_expert_out, router_logits\n",
    "    \n",
    "def test_DeepSeekMoE(): \n",
    "    x = torch.rand(2, 4, 16)\n",
    "    config = MoEconfig(2, 2, 16)\n",
    "    deep_seek_moe = DeepSeekMoE(config)\n",
    "    out = deep_seek_moe(x)\n",
    "    print(out[0].shape, out[1].shape)\n",
    "\n",
    "test_DeepSeekMoE()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 1.8591 (MSE: 1.8390, Aux: 2.0148)\n",
      "Batch 10, Loss: 1.6580 (MSE: 1.6379, Aux: 2.0058)\n",
      "Batch 20, Loss: 1.4946 (MSE: 1.4745, Aux: 2.0068)\n",
      "Batch 30, Loss: 1.3924 (MSE: 1.3724, Aux: 2.0051)\n",
      "Batch 40, Loss: 1.2909 (MSE: 1.2708, Aux: 2.0165)\n",
      "Batch 50, Loss: 1.2380 (MSE: 1.2179, Aux: 2.0148)\n",
      "Batch 60, Loss: 1.2032 (MSE: 1.1831, Aux: 2.0148)\n",
      "Batch 70, Loss: 1.1571 (MSE: 1.1369, Aux: 2.0175)\n",
      "Batch 80, Loss: 1.1068 (MSE: 1.0866, Aux: 2.0238)\n",
      "Batch 90, Loss: 1.0898 (MSE: 1.0696, Aux: 2.0190)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def switch_load_balancing_loss(router_logits: torch.Tensor, num_experts: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    计算 Switch Transformers 的负载均衡损失\n",
    "    \n",
    "    Args:\n",
    "        router_logits: shape [batch_size * sequence_length, num_experts]\n",
    "        num_experts: 专家数量\n",
    "    \n",
    "    Returns:\n",
    "        total_loss: 总损失 = auxiliary_loss + z_loss\n",
    "    \"\"\"\n",
    "    # 计算路由概率\n",
    "    router_probs = torch.softmax(router_logits, dim=-1)  # [b*s, num_experts]\n",
    "    \n",
    "    # 获取每个token的最优专家\n",
    "    _, selected_experts = torch.topk(router_probs, k=2, dim=-1) \n",
    "    \n",
    "    # 创建one-hot矩阵表示选中的专家\n",
    "    mask = torch.nn.functional.one_hot(selected_experts, num_experts).float() \n",
    "    \n",
    "    # 计算每个专家的期望负载 (理想情况下应该是 1/num_experts)\n",
    "    expected_load = torch.ones_like(router_probs) / num_experts\n",
    "    \n",
    "    # 计算实际负载 (每个专家处理的token数量除以总token数量)\n",
    "    # 在batch维度上计算平均值\n",
    "    actual_load = mask.mean(dim=0)\n",
    "    \n",
    "    # 计算auxiliary loss\n",
    "    # 这会惩罚负载分布与期望负载的差异\n",
    "    aux_loss = torch.sum(actual_load * router_probs.mean(dim=0)) * num_experts\n",
    "    \n",
    "    # 计算z_loss (可选)\n",
    "    # 这会惩罚过大的路由logits\n",
    "    z_loss = torch.mean(torch.square(router_logits))\n",
    "    z_loss_weight = 0.001  # 可调整的超参数\n",
    "    \n",
    "    # 总损失\n",
    "    total_loss = aux_loss + z_loss * z_loss_weight\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "def test_moe_training():\n",
    "    # Create a simple dataset\n",
    "    batch_size = 32\n",
    "    seq_len = 16\n",
    "    hidden_dim = 32\n",
    "    num_batches = 100\n",
    "    \n",
    "    # Initialize model and optimizer\n",
    "    config = MoEconfig(hidden_dim=hidden_dim, \n",
    "                      expert_num=4,\n",
    "                      top_k=2,\n",
    "                      share_expert_num=2)\n",
    "    model = DeepSeekMoE(config)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for batch in range(num_batches):\n",
    "        # Generate random input data\n",
    "        x = torch.randn(batch_size, seq_len, hidden_dim)\n",
    "        target = torch.randn(batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # Forward pass\n",
    "        output, router_logits = model(x)\n",
    "\n",
    "        # Compute losses\n",
    "        # MSE loss for prediction\n",
    "        mse_loss = F.mse_loss(output, target)\n",
    "        \n",
    "        aux_loss = switch_load_balancing_loss(router_logits, config.expert_num)\n",
    "        # Combined loss\n",
    "        total_loss = mse_loss + 0.01 * aux_loss\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 10 == 0:\n",
    "            print(f\"Batch {batch}, Loss: {total_loss.item():.4f} \"\n",
    "                  f\"(MSE: {mse_loss.item():.4f}, Aux: {aux_loss.item():.4f})\")\n",
    "\n",
    "# Run the training test\n",
    "test_moe_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
