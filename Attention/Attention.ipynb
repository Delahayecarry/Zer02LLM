{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attention 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "注意力分数为：\n",
      " tensor([[[ 0.8778,  0.1924,  0.2765, -0.4848],\n",
      "         [ 0.8742,  0.1992,  0.2778, -0.4821]],\n",
      "\n",
      "        [[ 1.0152,  0.1694,  0.2651, -0.4668],\n",
      "         [ 1.0153,  0.1681,  0.2641, -0.4667]],\n",
      "\n",
      "        [[ 1.0385,  0.0987,  0.2702, -0.6029],\n",
      "         [ 1.0400,  0.1005,  0.2747, -0.6045]]], grad_fn=<UnsafeViewBackward0>)\n",
      "注意力为：\n",
      " tensor([[[0.4380, 0.5620],\n",
      "         [0.4558, 0.5442]],\n",
      "\n",
      "        [[0.4680, 0.5320],\n",
      "         [0.4648, 0.5352]],\n",
      "\n",
      "        [[0.5236, 0.4764],\n",
      "         [0.5137, 0.4863]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8778,  0.1924,  0.2765, -0.4848],\n",
       "         [ 0.8742,  0.1992,  0.2778, -0.4821]],\n",
       "\n",
       "        [[ 1.0152,  0.1694,  0.2651, -0.4668],\n",
       "         [ 1.0153,  0.1681,  0.2641, -0.4667]],\n",
       "\n",
       "        [[ 1.0385,  0.0987,  0.2702, -0.6029],\n",
       "         [ 1.0400,  0.1005,  0.2747, -0.6045]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SelfAttentionv1(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(SelfAttentionv1, self).__init__()\n",
    "        self.hidden_dim = hidden_dim # 隐藏层维度\n",
    "        self.q_prj = nn.Linear(hidden_dim, hidden_dim) # 线性变换\n",
    "        self.k_prj = nn.Linear(hidden_dim, hidden_dim) # 线性变换\n",
    "        self.v_prj = nn.Linear(hidden_dim, hidden_dim) # 线性变换\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, hidden_dim]\n",
    "        Q = self.q_prj(x)\n",
    "        K = self.k_prj(x)\n",
    "        V = self.v_prj(x)\n",
    "\n",
    "        # Q: [batch_size, seq_len, hidden_dim]\n",
    "        # K: [batch_size, seq_len, hidden_dim]\n",
    "        # V: [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "        q_k = torch.matmul(Q, K.transpose(1, 2))\n",
    "        # q_k: [batch_size, seq_len, seq_len]\n",
    "        # K.transpose(1, 2): [batch_size, hidden_dim, seq_len]\n",
    "\n",
    "        attention_weights = F.softmax(q_k / torch.sqrt(torch.tensor(self.hidden_dim)), dim=-1)\n",
    "        # attention_weights: [batch_size, seq_len, seq_len]\n",
    "        attention_scroes = torch.matmul(attention_weights, V)\n",
    "        # attention_scroes: [batch_size, seq_len, hidden_dim]\n",
    "        print('注意力分数为：\\n', attention_scroes)\n",
    "        print('注意力为：\\n', attention_weights)\n",
    "        return attention_scroes\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "X = torch.rand(3, 2, 4)\n",
    "# X.shape: [batch_size, seq_len, hidden_dim]\n",
    "net = SelfAttentionv1(4) # 初始化 输入参数为隐藏层维度hidden_dim\n",
    "net(X)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attention -2(效率提升) -适用于小网络情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "注意力分数为：\n",
      " tensor([[[-0.7213,  0.4493, -0.5222, -0.0127],\n",
      "         [-0.7342,  0.4358, -0.5476, -0.0024]],\n",
      "\n",
      "        [[-0.7223,  0.4778, -0.5426, -0.1256],\n",
      "         [-0.7189,  0.4836, -0.5337, -0.1305]],\n",
      "\n",
      "        [[-0.5797,  0.5654, -0.2956, -0.0695],\n",
      "         [-0.5816,  0.5637, -0.2993, -0.0679]]], grad_fn=<UnsafeViewBackward0>)\n",
      "注意力为：\n",
      " tensor([[[0.5430, 0.4570],\n",
      "         [0.5207, 0.4793]],\n",
      "\n",
      "        [[0.4644, 0.5356],\n",
      "         [0.4475, 0.5525]],\n",
      "\n",
      "        [[0.5131, 0.4869],\n",
      "         [0.5042, 0.4958]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5543,  0.6297,  0.1301, -0.1723],\n",
       "         [ 0.5677,  0.6364,  0.1352, -0.1872]],\n",
       "\n",
       "        [[ 0.5232,  0.6501,  0.1275, -0.1786],\n",
       "         [ 0.5182,  0.6480,  0.1257, -0.1729]],\n",
       "\n",
       "        [[ 0.4433,  0.5613,  0.0788, -0.0422],\n",
       "         [ 0.4453,  0.5622,  0.0795, -0.0443]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SelfAttentionv2(nn.Module):\n",
    "    def __init__ (self, dim):\n",
    "        super(SelfAttentionv2, self).__init__( )\n",
    "        self.dim = dim\n",
    "        self.proj = nn.Linear(dim, dim * 3)\n",
    "        self.outputs = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, hidden_dim]\n",
    "        QKV = self.proj(x)\n",
    "        Q, K, V = torch.split(QKV, self.dim, dim=-1)\n",
    "        # Q, K, V: [batch_size, seq_len, hidden_dim]\n",
    "        attention_weights = F.softmax(torch.matmul(Q, K.transpose(1, 2) / torch.sqrt(torch.tensor(self.dim))), dim=-1)\n",
    "        attention_scores = torch.matmul(attention_weights, V)\n",
    "\n",
    "        outputs = self.outputs(attention_scores)\n",
    "        print('注意力分数为：\\n', attention_scores)\n",
    "        print('注意力为：\\n', attention_weights)\n",
    "        return outputs\n",
    "    \n",
    "X = torch.rand(3, 2, 4)\n",
    "net = SelfAttentionv2(4)\n",
    "net(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加入细节（dropout的加入）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3588,  0.5376,  0.0166,  0.4375],\n",
       "         [-0.3394,  0.4976, -0.0449,  0.3910],\n",
       "         [-0.3584,  0.5382,  0.0190,  0.4383],\n",
       "         [-0.3584,  0.5377,  0.0172,  0.4387]],\n",
       "\n",
       "        [[-0.3928,  0.4786,  0.0060,  0.4454],\n",
       "         [-0.4350,  0.4744, -0.0084,  0.4409],\n",
       "         [-0.4663,  0.5095,  0.0474,  0.4613],\n",
       "         [-0.3782,  0.4911, -0.0161,  0.4294]],\n",
       "\n",
       "        [[-0.3991,  0.6046, -0.0762,  0.3775],\n",
       "         [-0.3918,  0.5564, -0.0970,  0.3846],\n",
       "         [-0.3964,  0.6046, -0.0763,  0.3767],\n",
       "         [-0.3934,  0.5569, -0.0966,  0.3850]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SelfAttentionv2(nn.Module):\n",
    "    def __init__ (self, dim):\n",
    "        super(SelfAttentionv2, self).__init__( )\n",
    "        self.dim = dim\n",
    "        self.att_drop = nn.Dropout(0.1)\n",
    "        self.proj = nn.Linear(dim, dim * 3)\n",
    "        self.outputs = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # atten_mask: [batch_size, seq_len]\n",
    "        # x: [batch_size, seq_len, hidden_dim]\n",
    "        QKV = self.proj(x)\n",
    "        Q, K, V = torch.split(QKV, self.dim, dim=-1)\n",
    "        # Q, K, V: [batch_size, seq_len, hidden_dim]\n",
    "        attention_weights = torch.matmul(Q, K.transpose(1, 2) / torch.sqrt(torch.tensor(self.dim)))\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_weights.masked_fill(attention_mask == 0, float('-1e20'))\n",
    "        attention_scores = F.softmax(attention_weights, dim=-1)\n",
    "        # print('注意力分数为（不带drop）：\\n', attention_scores)\n",
    "        attention_scores_drop = self.att_drop(attention_scores)\n",
    "        outputs_drop = torch.matmul(attention_scores_drop, V)\n",
    "\n",
    "        outputs = self.outputs(outputs_drop)\n",
    "        # print('注意力分数为（带drop）：\\n', attention_scores_drop)\n",
    "        # print('注意力为（带drop）：\\n', attention_scores)\n",
    "        return outputs\n",
    "    \n",
    "X = torch.rand(3, 4, 4)\n",
    "b = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 0],\n",
    "        [1, 1, 0, 0],\n",
    "        [1, 0, 0, 0],\n",
    "    ]\n",
    ")\n",
    "print(b.shape)\n",
    "mask = b.unsqueeze(dim=1).repeat(1, 4, 1)\n",
    "\n",
    "net = SelfAttentionv2(4)\n",
    "net(X, mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attention-3 完整版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atten_weights为：\n",
      " tensor([[[ 6.0511e-02,  3.8119e-02,  8.7185e-02, -1.0000e+20],\n",
      "         [ 6.9878e-02,  4.0051e-02,  8.0080e-02, -1.0000e+20],\n",
      "         [ 2.0244e-02,  8.3548e-03,  4.3131e-02, -1.0000e+20],\n",
      "         [ 3.0784e-02,  2.3479e-02,  6.2891e-02, -1.0000e+20]],\n",
      "\n",
      "        [[ 1.2752e-02,  1.2739e-01, -1.0000e+20, -1.0000e+20],\n",
      "         [-1.3652e-01, -8.6551e-02, -1.0000e+20, -1.0000e+20],\n",
      "         [ 3.4274e-02,  7.9726e-02, -1.0000e+20, -1.0000e+20],\n",
      "         [-4.1723e-02,  2.5581e-02, -1.0000e+20, -1.0000e+20]],\n",
      "\n",
      "        [[ 4.3268e-02, -1.0000e+20, -1.0000e+20, -1.0000e+20],\n",
      "         [ 1.4062e-02, -1.0000e+20, -1.0000e+20, -1.0000e+20],\n",
      "         [-7.5964e-03, -1.0000e+20, -1.0000e+20, -1.0000e+20],\n",
      "         [-4.3503e-02, -1.0000e+20, -1.0000e+20, -1.0000e+20]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "注意力分数为：\n",
      " tensor([[[0.3328, 0.3254, 0.3418, 0.0000],\n",
      "         [0.3355, 0.3256, 0.3389, 0.0000],\n",
      "         [0.3321, 0.3282, 0.3398, 0.0000],\n",
      "         [0.3305, 0.3281, 0.3413, 0.0000]],\n",
      "\n",
      "        [[0.4714, 0.5286, 0.0000, 0.0000],\n",
      "         [0.4875, 0.5125, 0.0000, 0.0000],\n",
      "         [0.4886, 0.5114, 0.0000, 0.0000],\n",
      "         [0.4832, 0.5168, 0.0000, 0.0000]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2996, -0.2629,  0.2752, -0.1491],\n",
       "         [-0.1614, -0.4810,  0.4685,  0.0148],\n",
       "         [-0.1612, -0.4807,  0.4680,  0.0142],\n",
       "         [-0.2221, -0.3747,  0.3709, -0.0697]],\n",
       "\n",
       "        [[-0.3277, -0.2888,  0.2875, -0.1256],\n",
       "         [-0.3262, -0.2918,  0.2892, -0.1245],\n",
       "         [-0.1795, -0.4096,  0.3289, -0.1212],\n",
       "         [-0.1785, -0.4099,  0.3288, -0.1215]],\n",
       "\n",
       "        [[-0.2251, -0.4599,  0.4430, -0.0071],\n",
       "         [-0.2251, -0.4599,  0.4430, -0.0071],\n",
       "         [-0.2251, -0.4599,  0.4430, -0.0071],\n",
       "         [-0.2251, -0.4599,  0.4430, -0.0071]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SelfAttentionv3(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(SelfAttentionv3, self).__init__()\n",
    "        self.dim = dim  # 隐藏层维度\n",
    "        self.att_drop = nn.Dropout(0.1) # dropout\n",
    "        self.q_prj = nn.Linear(dim, dim) # Q矩阵\n",
    "        self.k_prj = nn.Linear(dim, dim) # K矩阵\n",
    "        self.v_prj = nn.Linear(dim, dim) # V矩阵\n",
    "        self.outputs_prj = nn.Linear(dim, dim) # 输出矩阵\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # x: [batch_size, seq_len, hidden_dim]\n",
    "        Q = self.q_prj(x)\n",
    "        K = self.k_prj(x)\n",
    "        V = self.v_prj(x)\n",
    "        # Q, K, V: [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "        # 计算注意力分数(未经过softmax)\n",
    "        atten_weights = torch.matmul(Q, K.transpose(1, 2) / torch.sqrt(torch.tensor(self.dim)))\n",
    "\n",
    "        # mask\n",
    "        if attention_mask is not None:\n",
    "            # mask: [batch_size, seq_len]\n",
    "            # 如果有mask，将mask为0的位置的权重设置为负无穷\n",
    "            atten_weights = atten_weights.masked_fill(attention_mask == 0, float('-1e20'))\n",
    "        print('atten_weights为：\\n', atten_weights)\n",
    "        atten_scores = F.softmax(atten_weights, dim=-1) # softmax\n",
    "        print('注意力分数为：\\n', atten_scores)\n",
    "\n",
    "        # dropout\n",
    "        atten_scores_drop = self.att_drop(atten_scores)\n",
    "\n",
    "        # 输出\n",
    "        outputs_drop = torch.matmul(atten_scores_drop, V)\n",
    "        outputs = self.outputs_prj(outputs_drop)\n",
    "\n",
    "        return outputs\n",
    "X = torch.rand(3, 4, 4)\n",
    "b = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 0],\n",
    "        [1, 1, 0, 0],\n",
    "        [1, 0, 0, 0],\n",
    "    ]\n",
    ")\n",
    "mask = b.unsqueeze(dim=1).repeat(1, 4, 1)\n",
    "# print(mask)\n",
    "net = SelfAttentionv3(4)\n",
    "net(X, mask)\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiheadattention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch    \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8, 4, 4])\n",
      "atten_weights为：\n",
      " tensor([[[[-0.0115,  0.0934,  0.1705,    -inf],\n",
      "          [ 0.0662,  0.0352,  0.1450,    -inf],\n",
      "          [ 0.0251,  0.0474,  0.1311,    -inf],\n",
      "          [ 0.0045,  0.0365,  0.0454,    -inf]],\n",
      "\n",
      "         [[ 0.0339, -0.1279, -0.0885,    -inf],\n",
      "          [-0.0651, -0.2255, -0.1660,    -inf],\n",
      "          [ 0.0286, -0.1330, -0.1031,    -inf],\n",
      "          [-0.0301, -0.2029, -0.1718,    -inf]],\n",
      "\n",
      "         [[-0.1100, -0.0994, -0.1472,    -inf],\n",
      "          [-0.0950, -0.0587, -0.0683,    -inf],\n",
      "          [-0.0690, -0.0657, -0.0617,    -inf],\n",
      "          [-0.0596, -0.1162, -0.0576,    -inf]],\n",
      "\n",
      "         [[-0.1514, -0.1138, -0.2064,    -inf],\n",
      "          [-0.2150, -0.2159, -0.2233,    -inf],\n",
      "          [-0.1203, -0.0865, -0.1552,    -inf],\n",
      "          [-0.2066, -0.2073, -0.2831,    -inf]],\n",
      "\n",
      "         [[ 0.0709, -0.0083,  0.0465,    -inf],\n",
      "          [ 0.0079,  0.0728, -0.0163,    -inf],\n",
      "          [ 0.0654,  0.1308,  0.0179,    -inf],\n",
      "          [ 0.1569,  0.0594,  0.0484,    -inf]],\n",
      "\n",
      "         [[-0.1362, -0.1271, -0.1799,    -inf],\n",
      "          [-0.0580, -0.1904, -0.1574,    -inf],\n",
      "          [ 0.0445, -0.0995, -0.1103,    -inf],\n",
      "          [ 0.0065, -0.0690, -0.1142,    -inf]],\n",
      "\n",
      "         [[ 0.1272, -0.0171,  0.0968,    -inf],\n",
      "          [ 0.2572,  0.0803,  0.2574,    -inf],\n",
      "          [ 0.0991, -0.0707,  0.0531,    -inf],\n",
      "          [ 0.1195,  0.0245,  0.0454,    -inf]],\n",
      "\n",
      "         [[ 0.0069, -0.0325,  0.0323,    -inf],\n",
      "          [-0.1085, -0.0149, -0.0233,    -inf],\n",
      "          [-0.0253, -0.0035, -0.0409,    -inf],\n",
      "          [ 0.0577, -0.0053,  0.0429,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0655,  0.0100,    -inf,    -inf],\n",
      "          [ 0.1428,  0.1184,    -inf,    -inf],\n",
      "          [ 0.0955,  0.1049,    -inf,    -inf],\n",
      "          [ 0.0671,  0.0171,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0728, -0.0493,    -inf,    -inf],\n",
      "          [ 0.0217,  0.0310,    -inf,    -inf],\n",
      "          [-0.1138, -0.1266,    -inf,    -inf],\n",
      "          [ 0.0061,  0.0404,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0141, -0.1609,    -inf,    -inf],\n",
      "          [-0.1983, -0.2040,    -inf,    -inf],\n",
      "          [-0.1742, -0.2447,    -inf,    -inf],\n",
      "          [-0.1856, -0.2310,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1499, -0.0256,    -inf,    -inf],\n",
      "          [-0.2991, -0.2005,    -inf,    -inf],\n",
      "          [-0.0712,  0.0019,    -inf,    -inf],\n",
      "          [-0.1857, -0.1939,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1081, -0.0073,    -inf,    -inf],\n",
      "          [ 0.0007, -0.1306,    -inf,    -inf],\n",
      "          [ 0.0782,  0.0884,    -inf,    -inf],\n",
      "          [ 0.0522,  0.0151,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0939, -0.0655,    -inf,    -inf],\n",
      "          [-0.1484, -0.1008,    -inf,    -inf],\n",
      "          [-0.1436, -0.1585,    -inf,    -inf],\n",
      "          [-0.2161, -0.2016,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0222,  0.1574,    -inf,    -inf],\n",
      "          [-0.0067,  0.0368,    -inf,    -inf],\n",
      "          [ 0.0514,  0.1561,    -inf,    -inf],\n",
      "          [ 0.0225,  0.1063,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1193, -0.1415,    -inf,    -inf],\n",
      "          [-0.1244, -0.2141,    -inf,    -inf],\n",
      "          [-0.0778, -0.0507,    -inf,    -inf],\n",
      "          [-0.0451, -0.0292,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.1564,    -inf,    -inf,    -inf],\n",
      "          [ 0.0492,    -inf,    -inf,    -inf],\n",
      "          [-0.1206,    -inf,    -inf,    -inf],\n",
      "          [-0.0252,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0241,    -inf,    -inf,    -inf],\n",
      "          [-0.0645,    -inf,    -inf,    -inf],\n",
      "          [ 0.0247,    -inf,    -inf,    -inf],\n",
      "          [ 0.0190,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0353,    -inf,    -inf,    -inf],\n",
      "          [-0.0687,    -inf,    -inf,    -inf],\n",
      "          [-0.0811,    -inf,    -inf,    -inf],\n",
      "          [ 0.0303,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0252,    -inf,    -inf,    -inf],\n",
      "          [-0.1426,    -inf,    -inf,    -inf],\n",
      "          [-0.0645,    -inf,    -inf,    -inf],\n",
      "          [-0.1464,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0177,    -inf,    -inf,    -inf],\n",
      "          [ 0.0378,    -inf,    -inf,    -inf],\n",
      "          [ 0.0537,    -inf,    -inf,    -inf],\n",
      "          [ 0.0007,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0566,    -inf,    -inf,    -inf],\n",
      "          [-0.0465,    -inf,    -inf,    -inf],\n",
      "          [-0.0413,    -inf,    -inf,    -inf],\n",
      "          [-0.0130,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0596,    -inf,    -inf,    -inf],\n",
      "          [ 0.0765,    -inf,    -inf,    -inf],\n",
      "          [ 0.1266,    -inf,    -inf,    -inf],\n",
      "          [ 0.1507,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0077,    -inf,    -inf,    -inf],\n",
      "          [-0.0997,    -inf,    -inf,    -inf],\n",
      "          [ 0.0136,    -inf,    -inf,    -inf],\n",
      "          [-0.0318,    -inf,    -inf,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "atten_scores为：\n",
      " tensor([[[[0.3021, 0.3355, 0.3624, 0.0000],\n",
      "          [0.3277, 0.3177, 0.3546, 0.0000],\n",
      "          [0.3190, 0.3262, 0.3547, 0.0000],\n",
      "          [0.3253, 0.3359, 0.3388, 0.0000]],\n",
      "\n",
      "         [[0.3656, 0.3110, 0.3235, 0.0000],\n",
      "          [0.3629, 0.3091, 0.3280, 0.0000],\n",
      "          [0.3666, 0.3119, 0.3214, 0.0000],\n",
      "          [0.3691, 0.3105, 0.3203, 0.0000]],\n",
      "\n",
      "         [[0.3362, 0.3398, 0.3240, 0.0000],\n",
      "          [0.3264, 0.3384, 0.3352, 0.0000],\n",
      "          [0.3322, 0.3333, 0.3346, 0.0000],\n",
      "          [0.3393, 0.3207, 0.3400, 0.0000]],\n",
      "\n",
      "         [[0.3350, 0.3479, 0.3171, 0.0000],\n",
      "          [0.3344, 0.3341, 0.3316, 0.0000],\n",
      "          [0.3333, 0.3448, 0.3219, 0.0000],\n",
      "          [0.3418, 0.3416, 0.3166, 0.0000]],\n",
      "\n",
      "         [[0.3449, 0.3186, 0.3365, 0.0000],\n",
      "          [0.3286, 0.3506, 0.3207, 0.0000],\n",
      "          [0.3310, 0.3534, 0.3156, 0.0000],\n",
      "          [0.3566, 0.3235, 0.3199, 0.0000]],\n",
      "\n",
      "         [[0.3371, 0.3402, 0.3227, 0.0000],\n",
      "          [0.3595, 0.3149, 0.3255, 0.0000],\n",
      "          [0.3673, 0.3180, 0.3146, 0.0000],\n",
      "          [0.3554, 0.3296, 0.3150, 0.0000]],\n",
      "\n",
      "         [[0.3526, 0.3053, 0.3421, 0.0000],\n",
      "          [0.3524, 0.2952, 0.3524, 0.0000],\n",
      "          [0.3573, 0.3015, 0.3412, 0.0000],\n",
      "          [0.3524, 0.3204, 0.3272, 0.0000]],\n",
      "\n",
      "         [[0.3348, 0.3218, 0.3434, 0.0000],\n",
      "          [0.3138, 0.3445, 0.3417, 0.0000],\n",
      "          [0.3326, 0.3399, 0.3275, 0.0000],\n",
      "          [0.3420, 0.3211, 0.3369, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5139, 0.4861, 0.0000, 0.0000],\n",
      "          [0.5061, 0.4939, 0.0000, 0.0000],\n",
      "          [0.4976, 0.5024, 0.0000, 0.0000],\n",
      "          [0.5125, 0.4875, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4941, 0.5059, 0.0000, 0.0000],\n",
      "          [0.4977, 0.5023, 0.0000, 0.0000],\n",
      "          [0.5032, 0.4968, 0.0000, 0.0000],\n",
      "          [0.4914, 0.5086, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.5366, 0.4634, 0.0000, 0.0000],\n",
      "          [0.5014, 0.4986, 0.0000, 0.0000],\n",
      "          [0.5176, 0.4824, 0.0000, 0.0000],\n",
      "          [0.5113, 0.4887, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4690, 0.5310, 0.0000, 0.0000],\n",
      "          [0.4754, 0.5246, 0.0000, 0.0000],\n",
      "          [0.4817, 0.5183, 0.0000, 0.0000],\n",
      "          [0.5020, 0.4980, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.5288, 0.4712, 0.0000, 0.0000],\n",
      "          [0.5328, 0.4672, 0.0000, 0.0000],\n",
      "          [0.4975, 0.5025, 0.0000, 0.0000],\n",
      "          [0.5093, 0.4907, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4929, 0.5071, 0.0000, 0.0000],\n",
      "          [0.4881, 0.5119, 0.0000, 0.0000],\n",
      "          [0.5037, 0.4963, 0.0000, 0.0000],\n",
      "          [0.4964, 0.5036, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4662, 0.5338, 0.0000, 0.0000],\n",
      "          [0.4891, 0.5109, 0.0000, 0.0000],\n",
      "          [0.4738, 0.5262, 0.0000, 0.0000],\n",
      "          [0.4790, 0.5210, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.5056, 0.4944, 0.0000, 0.0000],\n",
      "          [0.5224, 0.4776, 0.0000, 0.0000],\n",
      "          [0.4932, 0.5068, 0.0000, 0.0000],\n",
      "          [0.4960, 0.5040, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3772, -0.0779, -0.2052,  ..., -0.0576, -0.0062,  0.0177],\n",
       "         [ 0.3744, -0.0813, -0.2066,  ..., -0.0566, -0.0094,  0.0183],\n",
       "         [ 0.3736, -0.0817, -0.2060,  ..., -0.0572, -0.0072,  0.0196],\n",
       "         [ 0.3753, -0.0803, -0.2048,  ..., -0.0594, -0.0069,  0.0200]],\n",
       "\n",
       "        [[ 0.2413, -0.1107, -0.1947,  ..., -0.0364, -0.0559, -0.0388],\n",
       "         [ 0.2421, -0.1127, -0.1933,  ..., -0.0397, -0.0544, -0.0376],\n",
       "         [ 0.2460, -0.1129, -0.1936,  ..., -0.0401, -0.0508, -0.0384],\n",
       "         [ 0.2465, -0.1129, -0.1923,  ..., -0.0393, -0.0514, -0.0363]],\n",
       "\n",
       "        [[ 0.3836, -0.0500, -0.1790,  ..., -0.1062,  0.1555, -0.0078],\n",
       "         [ 0.3836, -0.0500, -0.1790,  ..., -0.1062,  0.1555, -0.0078],\n",
       "         [ 0.3836, -0.0500, -0.1790,  ..., -0.1062,  0.1555, -0.0078],\n",
       "         [ 0.3836, -0.0500, -0.1790,  ..., -0.1062,  0.1555, -0.0078]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Multiheadattention(nn.Module):\n",
    "    def __init__(self, dim, num_heads):\n",
    "        super(Multiheadattention, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.q_prj = nn.Linear(dim, dim)\n",
    "        self.k_prj = nn.Linear(dim, dim)\n",
    "        self.v_prj = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.outputs = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        # x: [batch_size, seq_len, hidden_dim]\n",
    "        Q = self.q_prj(x)\n",
    "        K = self.k_prj(x)\n",
    "        V = self.v_prj(x)\n",
    "        # Q, K, V: [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "        # shape 变成 （batch_size, num_head, seq_len, head_dim）\n",
    "        Q_heads = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K_heads = K.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V_heads = V.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        # Q_heads, K_heads, V_heads: [batch_size, num_heads, seq_len, head_dim]\n",
    "\n",
    "        # 计算注意力分数\n",
    "        atten_weights = torch.matmul(\n",
    "            Q_heads, K_heads.permute(0, 1, 3, 2) / torch.sqrt(torch.tensor(self.head_dim))\n",
    "            )\n",
    "        # atten_weights: [batch_size, num_heads, seq_len, seq_len]\n",
    "\n",
    "        # mask\n",
    "        if attention_mask is not None:\n",
    "            atten_weights = atten_weights.masked_fill(attention_mask == 0 , float('-inf'))\n",
    "        print('atten_weights为：\\n', atten_weights)\n",
    "        # softmax\n",
    "        atten_scores = F.softmax(atten_weights, dim=-1)\n",
    "        # atten_scores: [batch_size, num_heads, seq_len, seq_len]\n",
    "        print('atten_scores为：\\n', atten_scores)\n",
    "        \n",
    "        # dropout\n",
    "        atten_scores_drop  = self.dropout(atten_scores) \n",
    "        # atten_scores_drop: [batch_size, num_heads, seq_len, seq_len]\n",
    "        outputs = atten_scores @ V_heads\n",
    "        # outputs: [batch_size, num_heads, seq_len, head_dim]\n",
    "\n",
    "        # shape 变成 （batch_size, seq_len, hidden_dim）\n",
    "        outputs = outputs.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_len, self.dim)\n",
    "\n",
    "        outputs = self.outputs(outputs)\n",
    "    \n",
    "        return outputs\n",
    "\n",
    "X = torch.rand(3, 4, 128)\n",
    "b = (torch.tensor([\n",
    "    [1, 1, 1, 0],\n",
    "    [1, 1, 0, 0],\n",
    "    [1, 0, 0, 0],\n",
    "])\n",
    ".unsqueeze(dim=1)\n",
    ".unsqueeze(dim=2)\n",
    ".expand(3, 8, 4, 4)\n",
    ")\n",
    "mask = b\n",
    "print(b.shape)\n",
    "net = Multiheadattention(128, 8)  # 8个头\n",
    "\n",
    "net(X, mask)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
