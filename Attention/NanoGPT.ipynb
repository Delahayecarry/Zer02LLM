{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nanoGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x240d09d4ab0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import torch    \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass\n",
    "import tiktoken\n",
    "\n",
    "# 创建保存目录\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "torch.manual_seed(1024)\n",
    "# 对于分布式训练，需要设置不同的随机种子\n",
    "# torch.cuda.manual_seed_all(1024)\n",
    "# torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTconfig():\n",
    "    vocab_size: int = 50257\n",
    "    head_num: int = 12\n",
    "    hidden_dim: int = 768\n",
    "    num_layers: int = 6\n",
    "    max_seq_len: int = 512 # max sequence length = block_size\n",
    "    dropout: float = 0.1\n",
    "    head_dim: int = hidden_dim // head_num\n",
    "    batch_size: int = 12\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. single attention head\n",
    "class SingleDotAttention(nn.Module):\n",
    "    def __init__(self, GPTconfig):\n",
    "        super(SingleDotAttention, self).__init__()\n",
    "        self.hidden_dim = GPTconfig.hidden_dim  # 添加hidden_dim属性\n",
    "        self.head_dim = GPTconfig.head_dim\n",
    "        self.max_seq_len = GPTconfig.max_seq_len\n",
    "        self.dropout = GPTconfig.dropout\n",
    "\n",
    "        self.q = nn.Linear(GPTconfig.hidden_dim, GPTconfig.head_dim)\n",
    "        self.k = nn.Linear(GPTconfig.hidden_dim, GPTconfig.head_dim)\n",
    "        self.v = nn.Linear(GPTconfig.hidden_dim, GPTconfig.head_dim)\n",
    "        self.dropout = nn.Dropout(GPTconfig.dropout)\n",
    "        self.register_buffer(\n",
    "            \"Attention_mask\",\n",
    "            torch.tril(torch.ones(GPTconfig.max_seq_len, GPTconfig.max_seq_len))\n",
    "        ) # 崭新写法 # 下三角矩阵\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 输入x的形状: [batch_size, seq_len, hidden_dim]\n",
    "        q = self.q(x)  # [batch_size, seq_len, head_dim]\n",
    "        k = self.k(x)  # [batch_size, seq_len, head_dim]\n",
    "        v = self.v(x)  # [batch_size, seq_len, head_dim]\n",
    "\n",
    "        # 计算注意力分数\n",
    "        q_k = torch.matmul(q, k.transpose(-2, -1))  # [batch_size, seq_len, seq_len]\n",
    "        \n",
    "        # 获取当前序列长度并应用掩码 #动态掩码处理\n",
    "        seq_len = q.size(1)\n",
    "        mask = self.Attention_mask[:seq_len, :seq_len]\n",
    "        q_k_masked = q_k.masked_fill(mask == 0, float(\"-1e10\"))  # 掩盖未来信息\n",
    "        \n",
    "        # 使用head_dim而不是hidden_dim进行缩放，这是更准确的做法\n",
    "        # 因为head_dim是查询和键的维度\n",
    "        attention = F.softmax(q_k_masked / (self.head_dim ** 0.5), dim=-1)\n",
    "        \n",
    "        # 应用dropout到注意力权重\n",
    "        attention = self.dropout(attention)  # [batch_size, seq_len, seq_len]\n",
    "        \n",
    "        # 计算输出\n",
    "        sig_outputs = torch.matmul(attention, v)  # [batch_size, seq_len, head_dim]\n",
    "        \n",
    "        return sig_outputs\n",
    "\n",
    "#2 multi-head attention\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, GPTconfig):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        self.head_num = GPTconfig.head_num\n",
    "        self.head_dim = GPTconfig.head_dim\n",
    "        self.hidden_dim = GPTconfig.hidden_dim\n",
    "        \n",
    "        # 创建多个注意力头\n",
    "        self.attentions = nn.ModuleList(\n",
    "            [\n",
    "                SingleDotAttention(GPTconfig) for _ in range(self.head_num)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # 投影层，将多头拼接后的结果映射回hidden_dim\n",
    "        self.proj = nn.Linear(self.head_num * self.head_dim, self.hidden_dim)\n",
    "        self.dropout = nn.Dropout(GPTconfig.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 输入x的形状: [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # 并行处理所有注意力头\n",
    "        attention_heads = [\n",
    "            attention(x) for attention in self.attentions\n",
    "        ] # 多个attention head，每个形状为 [batch_size, seq_len, head_dim]\n",
    "\n",
    "        # 沿最后一个维度拼接所有注意力头的输出\n",
    "        concat_atten = torch.cat(attention_heads, dim=-1)\n",
    "        # concat_atten形状: [batch_size, seq_len, head_num * head_dim]\n",
    "\n",
    "        # 通过投影层映射回原始维度\n",
    "        outputs = self.proj(concat_atten)\n",
    "        \n",
    "        # 应用dropout并返回\n",
    "        mth_outputs = self.dropout(outputs)\n",
    "        # 输出形状: [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "        return mth_outputs\n",
    "    \n",
    "#3. feed forward\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, GPTconfig):\n",
    "        super(FFN, self).__init__()\n",
    "        self.hidden_dim = GPTconfig.hidden_dim\n",
    "        \n",
    "        # 前馈网络的上投影层，扩展维度为原来的4倍\n",
    "        self.up = nn.Linear(self.hidden_dim, self.hidden_dim * 4)\n",
    "        \n",
    "        # 激活函数，使用GELU而不是ReLU，这是现代Transformer的常见选择\n",
    "        self.mid = nn.GELU()\n",
    "        \n",
    "        # 下投影层，将维度映射回原始hidden_dim\n",
    "        self.down = nn.Linear(self.hidden_dim * 4, self.hidden_dim)\n",
    "        \n",
    "        # dropout层，用于防止过拟合\n",
    "        self.dropout = nn.Dropout(GPTconfig.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 输入x的形状: [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # 完整的前馈网络流程: 上投影 -> 激活 -> 下投影 -> dropout\n",
    "        ffn_outputs = self.dropout(self.down(self.mid(self.up(x))))\n",
    "        # 输出形状: [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        return ffn_outputs\n",
    "    \n",
    "#4. block\n",
    "class Block(nn.Module):\n",
    "    def __init__ (self, GPTconfig):\n",
    "        super(Block, self).__init__()\n",
    "        # 多头自注意力层\n",
    "        self.attention = MultiheadAttention(GPTconfig)\n",
    "        \n",
    "        # 前馈网络层\n",
    "        self.FFN = FFN(GPTconfig)\n",
    "        \n",
    "        # 两个层归一化层，用于注意力子层和前馈网络子层\n",
    "        self.layernorm1 = nn.LayerNorm(GPTconfig.hidden_dim) # 这里使用PyTorch内置的LayerNorm\n",
    "        self.layernorm2 = nn.LayerNorm(GPTconfig.hidden_dim)\n",
    "        \n",
    "        # dropout层，用于残差连接\n",
    "        self.dropout = nn.Dropout(GPTconfig.dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 输入x的形状: [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # 第一个子层: 多头自注意力 (带残差连接和层归一化)\n",
    "        # 先应用层归一化，再应用注意力（Pre-LN架构）\n",
    "        attn_output = self.attention(self.layernorm1(x))\n",
    "        x = x + self.dropout(attn_output)  # 应用dropout到残差路径\n",
    "        \n",
    "        # 第二个子层: 前馈网络 (带残差连接和层归一化)\n",
    "        ffn_output = self.FFN(self.layernorm2(x))\n",
    "        x = x + self.dropout(ffn_output)  # 应用dropout到残差路径\n",
    "        \n",
    "        # 输出形状: [batch_size, seq_len, hidden_dim]\n",
    "        return x\n",
    "    \n",
    "#5. GPT\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, GPTconfig):\n",
    "        super(GPT, self).__init__()\n",
    "        # 保存最大序列长度，用于位置编码和生成\n",
    "        self.max_seq_len = GPTconfig.max_seq_len\n",
    "        \n",
    "        # 词嵌入表，将token ID映射为向量\n",
    "        self.token_emb_table = nn.Embedding(GPTconfig.vocab_size, GPTconfig.hidden_dim)\n",
    "        \n",
    "        # 位置嵌入表，为每个位置提供一个可学习的向量\n",
    "        self.pos_emb_table = nn.Embedding(GPTconfig.max_seq_len, GPTconfig.hidden_dim)\n",
    "        \n",
    "        # Transformer块的序列\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(GPTconfig) for _ in range(GPTconfig.num_layers)]\n",
    "        )\n",
    "        \n",
    "        # 最终的层归一化\n",
    "        self.layernorm = nn.LayerNorm(GPTconfig.hidden_dim)\n",
    "        \n",
    "        # 语言模型头，将hidden_dim映射回vocab_size\n",
    "        self.ln_head = nn.Linear(GPTconfig.hidden_dim, GPTconfig.vocab_size)\n",
    "\n",
    "        # 权重绑定：使token嵌入和语言模型头共享权重\n",
    "        # 这是一种常见的优化，可以减少参数数量并提高性能\n",
    "        self.token_emb_table.weight = self.ln_head.weight\n",
    "        \n",
    "        # 应用权重初始化\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"初始化模型权重\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # 线性层使用正态分布初始化\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                # 偏置初始化为零\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            # 嵌入层使用正态分布初始化\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.padding_idx is not None:\n",
    "                # 如果有padding_idx，将其初始化为零\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "\n",
    "    def forward(self, idx, target=None):\n",
    "        \"\"\"\n",
    "        前向传播函数\n",
    "        \n",
    "        参数:\n",
    "            idx: 输入token索引，形状为[batch_size, seq_len]\n",
    "            target: 目标token索引，形状为[batch_size, seq_len]，用于计算损失\n",
    "            \n",
    "        返回:\n",
    "            logits: 预测的logits，形状为[batch_size, seq_len, vocab_size]\n",
    "            loss: 如果提供了target，则返回计算的损失；否则返回None\n",
    "        \"\"\"\n",
    "        # 获取batch_size和seq_len\n",
    "        batch_size, seq_len = idx.size()\n",
    "        \n",
    "        # 确保序列长度不超过模型的最大序列长度\n",
    "        if seq_len > self.max_seq_len:\n",
    "            idx = idx[:, -self.max_seq_len:]  # 截取最后max_seq_len个token\n",
    "            if target is not None:\n",
    "                target = target[:, -self.max_seq_len:]\n",
    "            seq_len = self.max_seq_len\n",
    "        \n",
    "        # 获取词嵌入\n",
    "        token_emb = self.token_emb_table(idx)  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # 生成位置编码\n",
    "        pos = torch.arange(0, seq_len, device=idx.device)  # 确保从0开始\n",
    "        pos_emb = self.pos_emb_table(pos)  # [seq_len, hidden_dim]\n",
    "        \n",
    "        # 广播位置编码以匹配batch维度\n",
    "        pos_emb = pos_emb.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        # pos_emb形状: [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # 将词嵌入和位置编码相加\n",
    "        x = token_emb + pos_emb\n",
    "        \n",
    "        # 通过Transformer块\n",
    "        x = self.blocks(x)\n",
    "        \n",
    "        # 应用最终的层归一化\n",
    "        x = self.layernorm(x)\n",
    "        \n",
    "        # 计算logits\n",
    "        logits = self.ln_head(x)  # [batch_size, seq_len, vocab_size]\n",
    "        \n",
    "        # 如果没有提供target，只返回logits\n",
    "        if target is None:\n",
    "            return logits, None\n",
    "        \n",
    "        # 计算损失\n",
    "        batch_size, seq_len, vocab_size = logits.size()\n",
    "        logits_view = logits.view(batch_size * seq_len, vocab_size)\n",
    "        target = target.view(batch_size * seq_len)\n",
    "        loss = F.cross_entropy(logits_view, target)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
    "        \"\"\"\n",
    "        自回归生成新的token序列\n",
    "        \n",
    "        参数:\n",
    "            idx: 初始token序列，形状为[batch_size, seq_len]\n",
    "            max_new_tokens: 要生成的新token数量\n",
    "            temperature: 采样温度，控制生成的随机性。较高的温度会产生更多样化的输出，\n",
    "                        较低的温度会使输出更确定性。默认为1.0\n",
    "                        \n",
    "        返回:\n",
    "            生成后的完整token序列，形状为[batch_size, seq_len + max_new_tokens]\n",
    "        \"\"\"\n",
    "        # 保存原始idx以便后续操作\n",
    "        original_idx = idx.clone()\n",
    "        \n",
    "        # 逐个生成新token\n",
    "        for _ in range(max_new_tokens):\n",
    "            # 处理序列长度，如果超过最大长度则截断\n",
    "            idx_cond = idx if idx.size(1) <= self.max_seq_len else idx[:, -self.max_seq_len:]\n",
    "            \n",
    "            # 获取模型预测\n",
    "            logits, _ = self(idx_cond)\n",
    "            \n",
    "            # 只关注最后一个时间步的预测结果\n",
    "            logits = logits[:, -1, :]  # [batch_size, vocab_size]\n",
    "            \n",
    "            # 应用温度缩放，调整分布的平滑度\n",
    "            # 较高的温度会使分布更平滑，增加多样性；较低的温度会使分布更尖锐，增加确定性\n",
    "            logits = logits / temperature\n",
    "            \n",
    "            # 计算概率分布\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            # 从概率分布中采样下一个token\n",
    "            next_token = torch.multinomial(probs, num_samples=1)  # [batch_size, 1]\n",
    "            \n",
    "            # 将新token添加到序列中\n",
    "            idx = torch.cat([idx, next_token], dim=1)\n",
    "            \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. 数据加载\n",
    "class MyDataset(Dataset):\n",
    "    def __init__ (self, path, seq_len=512):\n",
    "        \n",
    "        self.enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.eos_token = self.enc.encode(\n",
    "            \"<|endoftext|>\",\n",
    "            allowed_special={\"<|endoftext|>\"}\n",
    "        )[0] # 利用特殊符号分割不同的训练样本 #GPT用的是<|endoftext|>\n",
    "\n",
    "        import json\n",
    "        self.encoded_data = []\n",
    "\n",
    "        self.max_lines = 1000\n",
    "        raw_data = []\n",
    "        with open(path, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= self.max_lines:\n",
    "                    break\n",
    "                try:\n",
    "                    text = json.loads(line.strip())['text']\n",
    "                    raw_data.append(text)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        full_encoded = []\n",
    "        for text in raw_data:\n",
    "            encoded_text = self.enc.encode(text)\n",
    "            full_encoded.extend(encoded_text + [self.eos_token])\n",
    "        \n",
    "        # 将长文本分割成训练样本\n",
    "        for i in range(0, len(full_encoded), self.seq_len):\n",
    "            # 多取一个 Token 作为目标\n",
    "            chunk = full_encoded[i:i+self.seq_len+1]\n",
    "            # 如果长度不够，用 eos_token 填充\n",
    "            if len(chunk) < self.seq_len + 1:\n",
    "                chunk = chunk + [self.eos_token] * (self.seq_len + 1 - len(chunk))\n",
    "            self.encoded_data.append(chunk)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encoded_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.encoded_data[idx]\n",
    "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"将文本编码为token IDs\"\"\"\n",
    "        return self.enc.encode(text)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"将token IDs解码为文本\"\"\"\n",
    "        return self.enc.decode(ids)\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_data)\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.encoded_data[idx]\n",
    "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "    def encode(self, text):\n",
    "        \"\"\"将文本编码为token IDs\"\"\"\n",
    "        return self.enc.encode(text)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"将token IDs解码为文本\"\"\"\n",
    "        return self.enc.decode(ids)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "train_dataset = MyDataset(r'your-data-path')\n",
    "\n",
    "# split traindataset to train and val\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [0.9, 0.1])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=12, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=12, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 512]) torch.Size([12, 512])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 120.166993 M\n"
     ]
    }
   ],
   "source": [
    "model = GPT(GPTconfig())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 打印一下参数量\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params / 1e6} M\")\n",
    "\n",
    "# 优化\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "# 设置cosine学习率调度器\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.15 GiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 19.96 GiB is allocated by PyTorch, and 31.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# 训练\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m---> 31\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(model, device, val_loader)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[74], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, device, train_loader, optimizer, scheduler)\u001b[0m\n\u001b[0;32m      6\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 8\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32me:\\xiangmu\\配套作业\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\xiangmu\\配套作业\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[69], line 135\u001b[0m, in \u001b[0;36mGPT.forward\u001b[1;34m(self, idx, target)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# [batch_size, seq_len, hidden_dim]\u001b[39;00m\n\u001b[0;32m    134\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(x)\n\u001b[1;32m--> 135\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# [batch_size, seq_len, vocab_size]\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32me:\\xiangmu\\配套作业\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\xiangmu\\配套作业\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32me:\\xiangmu\\配套作业\\.conda\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.15 GiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 19.96 GiB is allocated by PyTorch, and 31.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# 训练循环\n",
    "def train(model, device, train_loader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for idx, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(x, target = y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "        if idx % 100 == 0:\n",
    "            print(f'Epoch: {epoch}, Loss: {loss.item():.4f}')\n",
    "    return total_loss\n",
    "\n",
    "# 验证循环\n",
    "def eval(model, device, val_loader):\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits, loss = model(x, target = y)\n",
    "            eval_loss += loss.item()\n",
    "    \n",
    "    return eval_loss \n",
    "\n",
    "# 训练\n",
    "for epoch in range(10):\n",
    "    train_loss = train(model, device, train_loader, optimizer, scheduler)\n",
    "    val_loss = eval(model, device, val_loader)\n",
    "    print(f'Epoch: {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    # 保存模型\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    checkpoints = {\n",
    "        'epoch': epoch,\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict(),\n",
    "        'val_loss': avg_val_loss\n",
    "    }\n",
    "    # 保存每个epoch的模型\n",
    "    torch.save(checkpoints, f'checkpoints/epoch_{epoch}.pt')\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
