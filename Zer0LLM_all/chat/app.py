import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import random
import numpy as np
import re

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="Zero2LLM Chat",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# è®¾ç½®æ ·å¼
st.markdown("""
    <style>
        /* æ·»åŠ æ“ä½œæŒ‰é’®æ ·å¼ */
        .stButton button {
            border-radius: 50% !important;
            width: 32px !important;
            height: 32px !important;
            padding: 0 !important;
            background-color: transparent !important;
            border: 1px solid #ddd !important;
            display: flex !important;
            align-items: center !important;
            justify-content: center !important;
            font-size: 14px !important;
            color: #666 !important;
            margin: 5px 10px 5px 0 !important;
        }
        .stButton button:hover {
            border-color: #999 !important;
            color: #333 !important;
            background-color: #f5f5f5 !important;
        }
        .stMainBlockContainer > div:first-child {
            margin-top: -50px !important;
        }
        .stApp > div:last-child {
            margin-bottom: -35px !important;
        }
        
        /* é‡ç½®æŒ‰é’®åŸºç¡€æ ·å¼ */
        .stButton > button {
            all: unset !important;
            box-sizing: border-box !important;
            border-radius: 50% !important;
            width: 18px !important;
            height: 18px !important;
            min-width: 18px !important;
            min-height: 18px !important;
            max-width: 18px !important;
            max-height: 18px !important;
            padding: 0 !important;
            background-color: transparent !important;
            border: 1px solid #ddd !important;
            display: flex !important;
            align-items: center !important;
            justify-content: center !important;
            font-size: 14px !important;
            color: #888 !important;
            cursor: pointer !important;
            transition: all 0.2s ease !important;
            margin: 0 2px !important;
        }
    </style>
""", unsafe_allow_html=True)

# å…¨å±€å˜é‡
system_prompt = []
# device = "cuda" if torch.cuda.is_available() else "cpu"
device = "cpu" 

# æ¨¡å‹è·¯å¾„æ˜ å°„
MODEL_PATHS = {
    "Zero2LLM-v1-pretrain-0.02B": ["../Zer02LLM-v1-pretrain-0.02B", "Zero2LLM-v1-pretrain-0.02B"],
    "Zero2LLM-v1-sft-0.02B": ["../Zer02LLM-v1-sft-0.02B", "Zero2LLM-v1-sft-0.02B"],
    "Zero2LLM-v1-dpo-0.02B": ["../Zer02LLM-v1-dpo-0.02B", "Zero2LLM-v1-dpo-0.02B"]
}

def setup_seed(seed):
    """è®¾ç½®éšæœºç§å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

@st.cache_resource
def load_model_tokenizer(model_path):
    """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        use_fast=False,
        trust_remote_code=True
    )
    model = model.eval().to(device)
    return model, tokenizer

def clear_chat_messages():
    """æ¸…ç©ºèŠå¤©è®°å½•"""
    del st.session_state.messages
    del st.session_state.chat_messages

def init_chat_messages():
    """åˆå§‹åŒ–èŠå¤©æ¶ˆæ¯"""
    if "messages" not in st.session_state:
        st.session_state.messages = []
        st.session_state.chat_messages = []

def main():
    # ä¾§è¾¹æ é…ç½®
    st.sidebar.title("æ¨¡å‹è®¾å®šè°ƒæ•´")
    st.sidebar.text("ã€æ³¨ã€‘å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ä»¥ä¸‹å‚æ•°")
    
    # æ¨¡å‹å‚æ•°è®¾ç½®
    st.session_state.history_chat_num = st.sidebar.slider("å†å²å¯¹è¯è½®æ•°", 0, 6, 0, step=2)
    st.session_state.max_new_tokens = st.sidebar.slider("æœ€å¤§åºåˆ—é•¿åº¦", 256, 8192, 2048, step=256)
    st.session_state.top_p = st.sidebar.slider("Top-P", 0.8, 0.99, 0.9, step=0.01)
    st.session_state.temperature = st.sidebar.slider("Temperature", 0.6, 1.2, 0.7, step=0.01)

    # é€‰æ‹©æ¨¡å‹
    selected_model = st.sidebar.selectbox('é€‰æ‹©æ¨¡å‹', list(MODEL_PATHS.keys()), index=0)
    model_path = MODEL_PATHS[selected_model][0]
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer = load_model_tokenizer(model_path)

    # é¡µé¢æ ‡é¢˜
    st.title(f"ğŸ¤– {MODEL_PATHS[selected_model][1]} Chat")
    st.markdown('<p style="color: #666; font-style: italic;">å†…å®¹å®Œå…¨ç”±AIç”Ÿæˆï¼Œè¯·åŠ¡å¿…ä»”ç»†ç”„åˆ«</p>', unsafe_allow_html=True)

    # åˆå§‹åŒ–æ¶ˆæ¯
    init_chat_messages()
    messages = st.session_state.messages

    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for i, message in enumerate(messages):
        if message["role"] == "assistant":
            with st.chat_message("assistant"):
                st.markdown(message["content"], unsafe_allow_html=True)
                if st.button("Ã—", key=f"delete_{i}"):
                    st.session_state.messages = st.session_state.messages[:i-1]
                    st.session_state.chat_messages = st.session_state.chat_messages[:i-1]
                    st.rerun()
        else:
            st.markdown(
                f'<div style="display: flex; justify-content: flex-end;"><div style="display: inline-block; margin: 10px 0; padding: 8px 12px 8px 12px; background-color: gray; border-radius: 10px; color: white;">{message["content"]}</div></div>',
                unsafe_allow_html=True
            )

    # ç”¨æˆ·è¾“å…¥
    user_input = st.chat_input("è¾“å…¥æ¶ˆæ¯...")

    if user_input:
        # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        st.markdown(
            f'<div style="display: flex; justify-content: flex-end;"><div style="display: inline-block; margin: 10px 0; padding: 8px 12px 8px 12px; background-color: gray; border-radius: 10px; color: white;">{user_input}</div></div>',
            unsafe_allow_html=True
        )
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²è®°å½•
        messages.append({"role": "user", "content": user_input})
        st.session_state.chat_messages.append({"role": "user", "content": user_input})

        # ç”Ÿæˆå›å¤
        with st.chat_message("assistant"):
            placeholder = st.empty()
            
            # è®¾ç½®éšæœºç§å­
            random_seed = random.randint(0, 2**32 - 1)
            setup_seed(random_seed)

            # å‡†å¤‡è¾“å…¥
            st.session_state.chat_messages = system_prompt + st.session_state.chat_messages[-(st.session_state.history_chat_num + 1):]
            new_prompt = tokenizer.apply_chat_template(
                st.session_state.chat_messages,
                tokenize=False,
                add_generation_prompt=True
            )[-(st.session_state.max_new_tokens - 1):]

            # ç”Ÿæˆå›å¤
            x = torch.tensor(tokenizer(new_prompt)['input_ids'], device=device).unsqueeze(0)
            with torch.no_grad():
                try:
                    for y in model.generate(
                        x,
                        tokenizer.eos_token_id,
                        max_new_tokens=st.session_state.max_new_tokens,
                        temperature=st.session_state.temperature,
                        top_p=st.session_state.top_p,
                        stream=True
                    ):
                        answer = tokenizer.decode(y[0].tolist(), skip_special_tokens=True)
                        if (answer and answer[-1] == '') or not answer:
                            continue
                        placeholder.markdown(answer, unsafe_allow_html=True)
                except Exception as e:
                    st.error(f"ç”Ÿæˆå›å¤æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                    return

            # ä¿å­˜åŠ©æ‰‹å›å¤
            assistant_answer = answer.replace(new_prompt, "")
            messages.append({"role": "assistant", "content": assistant_answer})
            st.session_state.chat_messages.append({"role": "assistant", "content": assistant_answer})

if __name__ == "__main__":
    main() 