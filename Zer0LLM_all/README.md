# Zer02LLM_all

ä¸€ä¸ªè½»é‡çº§çš„ LLM (Large Language Model) å®ç°é¡¹ç›®ï¼Œä¸“æ³¨äºæ¨¡å‹è®­ç»ƒå’Œæ¨ç†ä¼˜åŒ–ã€‚

## é¡¹ç›®ç‰¹ç‚¹

- ğŸš€ é«˜æ•ˆçš„ Transformer æ¶æ„å®ç°
- ğŸ¯ æ”¯æŒ MoE (Mixture of Experts) ç»“æ„
- ğŸ’¡ å®ç°äº† Flash Attention ä¼˜åŒ–
- ğŸ”„ æ”¯æŒ RoPE (Rotary Position Embedding) ä½ç½®ç¼–ç 
- ğŸ“¦ æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ‰©å±•
- ğŸ›  å†…ç½®æ€§èƒ½ä¼˜åŒ–æœºåˆ¶
- ğŸ”¤ æ”¯æŒè‡ªå®šä¹‰ tokenizer

## ç›®å½•ç»“æ„

```
Zer02LLM_all/
â”œâ”€â”€ model/              # æ¨¡å‹ç›¸å…³ä»£ç 
â”‚   â”œâ”€â”€ model.py       # æ ¸å¿ƒæ¨¡å‹å®ç°
â”‚   â””â”€â”€ LLMconfig.py   # æ¨¡å‹é…ç½®ç±»
â”œâ”€â”€ tokenizer/         # tokenizer ç›¸å…³æ–‡ä»¶
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”œâ”€â”€ vocab.json
â”‚   â””â”€â”€ merges.txt
â”œâ”€â”€ datasets.py        # æ•°æ®é›†å¤„ç†
â”œâ”€â”€ pretrain_sft_lora.py  # è®­ç»ƒè„šæœ¬
â””â”€â”€ README.md
```

## æ ¸å¿ƒåŠŸèƒ½

- **é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶**: å®ç°äº†åŒ…å« Flash Attention åœ¨å†…çš„ä¼˜åŒ–æ³¨æ„åŠ›è®¡ç®—
- **MoE ä¸“å®¶ç³»ç»Ÿ**: æ”¯æŒåŠ¨æ€è·¯ç”±å’Œä¸“å®¶é€‰æ‹©
- **ä½ç½®ç¼–ç ä¼˜åŒ–**: é‡‡ç”¨ RoPE è¿›è¡Œä½ç½®ä¿¡æ¯ç¼–ç 
- **çµæ´»çš„æ¨¡å‹é…ç½®**: æ”¯æŒè‡ªå®šä¹‰æ¨¡å‹å‚æ•°å’Œç»“æ„
- **æµå¼ç”Ÿæˆ**: æ”¯æŒæ–‡æœ¬çš„æµå¼ç”Ÿæˆè¾“å‡º
- **è‡ªå®šä¹‰ Tokenizer**: æ”¯æŒä½¿ç”¨è‡ªå®šä¹‰çš„ tokenizer è¿›è¡Œè®­ç»ƒ

## ç¯å¢ƒå‡†å¤‡

### å¿…è¦æ–‡ä»¶å’Œç›®å½•
- `./tokenizer/` - tokenizerç›®å½•ï¼ˆåŒ…å«å¿…è¦çš„tokenizeræ–‡ä»¶ï¼‰
- `./dataset/pretrain_hq.jsonl` - é¢„è®­ç»ƒæ•°æ®æ–‡ä»¶
- `./out/` - æ¨¡å‹è¾“å‡ºç›®å½•ï¼ˆä¼šè‡ªåŠ¨åˆ›å»ºï¼‰

### å®‰è£…ä¾èµ–
```bash
pip install -r requirements.txt
```

## è®­ç»ƒæŒ‡å—

### åŸºç¡€è®­ç»ƒå‘½ä»¤

```bash
# CPUè®­ç»ƒï¼ˆæµ‹è¯•ç”¨ï¼‰
python pretrain_sft_lora.py --device cpu --mode pretrain --batch_size 2 --epochs 1 --dim 128 --n_layers 2 --max_seq_len 128 --n_heads 4 --data_path ./datasets/pretrain_hq.jsonl

# GPUé¢„è®­ç»ƒ
python pretrain_sft_lora.py --mode pretrain --batch_size 32 --epochs 1 --learning_rate 5e-4 --dim 512 --n_layers 8 --data_path ./datasets/pretrain_hq.jsonl

# SFTå¾®è°ƒ
python pretrain_sft_lora.py --mode sft --batch_size 32 --epochs 1 --learning_rate 5e-4 --dim 512 --n_layers 8 --data_path ./datasets/sft_data.jsonl

# MoEæ¨¡å‹è®­ç»ƒ
python pretrain_sft_lora.py --mode pretrain --use_moe --n_routed_experts 8 --num_experts_per_tok 2 --batch_size 32 --dim 512 --n_layers 8

# åˆ†å¸ƒå¼è®­ç»ƒ
torchrun --nproc_per_node=2 pretrain_sft_lora.py --mode pretrain --ddp --batch_size 16
```

### æ˜¾å­˜ä¼˜åŒ–é…ç½®

```bash
# 16GBæ˜¾å­˜é…ç½®
python pretrain_sft_lora.py --mode pretrain --dim 512 --n_layers 8 --batch_size 16 --accumulation_steps 16

# 24GBæ˜¾å­˜é…ç½®
python pretrain_sft_lora.py --mode pretrain --dim 768 --n_layers 12 --batch_size 24 --accumulation_steps 8

# 40GB+æ˜¾å­˜é…ç½®
python pretrain_sft_lora.py --mode pretrain --dim 1024 --n_layers 16 --batch_size 32 --accumulation_steps 4
```

### é‡è¦å‚æ•°è¯´æ˜

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| --dim | æ¨¡å‹ç»´åº¦ | 512 |
| --n_layers | æ¨¡å‹å±‚æ•° | 8 |
| --max_seq_len | æœ€å¤§åºåˆ—é•¿åº¦ | 512 |
| --batch_size | æ‰¹æ¬¡å¤§å° | 32 |
| --accumulation_steps | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° | 8 |
| --learning_rate | å­¦ä¹ ç‡ | 5e-4 |

### è®­ç»ƒç›‘æ§
- ä½¿ç”¨wandbç›‘æ§ï¼š`--use_wandb`
- æ—¥å¿—é—´éš”ï¼š`--log_interval`
- ä¿å­˜é—´éš”ï¼š`--save_interval`

### æ¨¡å‹ä¿å­˜
- é¢„è®­ç»ƒæ¨¡å‹ï¼š`out/pretrain_{dim}.pth`
- SFTæ¨¡å‹ï¼š`out/sft_{dim}.pth`
- MoEæ¨¡å‹ï¼šæ·»åŠ `_moe`åç¼€

## å¸¸è§é—®é¢˜è§£å†³

### 1. æ˜¾å­˜ä¸è¶³
- å‡å° batch_size
- å¢åŠ  accumulation_steps
- å‡å°æ¨¡å‹ç»´åº¦æˆ–å±‚æ•°
- ä½¿ç”¨ float16/bfloat16 ç²¾åº¦

### 2. è®­ç»ƒé€Ÿåº¦ä¼˜åŒ–
- å¯ç”¨ Flash Attention
- ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
- ä¼˜åŒ–æ•°æ®åŠ è½½ï¼ˆå¢åŠ  num_workersï¼‰

### 3. è®­ç»ƒç¨³å®šæ€§
- è°ƒæ•´å­¦ä¹ ç‡
- ä½¿ç”¨ warmup
- å¯ç”¨æ¢¯åº¦è£å‰ª

## å¼€æºåè®®

æœ¬é¡¹ç›®é‡‡ç”¨ MIT åè®® - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

