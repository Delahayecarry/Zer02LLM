# Zer02LLM

ä¸€ä¸ªè½»é‡çº§çš„å¤§è¯­è¨€æ¨¡å‹(LLM)è®­ç»ƒå’Œæ¨ç†æ¡†æ¶ï¼Œä¸“æ³¨äºé«˜æ•ˆå®ç°å’Œæ€§èƒ½ä¼˜åŒ–ã€‚

## é¡¹ç›®ç‰¹ç‚¹

- ğŸš€ é«˜æ•ˆçš„ Transformer æ¶æ„å®ç°
- ğŸ¯ æ”¯æŒ MoE (Mixture of Experts) ç»“æ„
- ğŸ’¡ å®ç°äº† Flash Attention ä¼˜åŒ–
- ğŸ”„ æ”¯æŒ RoPE (Rotary Position Embedding) ä½ç½®ç¼–ç 
- ğŸ“¦ æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ‰©å±•
- ğŸ›  å†…ç½®æ€§èƒ½ä¼˜åŒ–æœºåˆ¶
- ğŸ”¤ æ”¯æŒè‡ªå®šä¹‰ tokenizer
- ğŸ“Š å¼ºå¤§çš„ Wandb å®éªŒè¿½è¸ªä¸å¯è§†åŒ–
- ğŸ”„ æ”¯æŒæµå¼æ¨ç†è¾“å‡º
- ğŸ’¾ æ”¯æŒæ¢¯åº¦ç´¯ç§¯å’Œæ··åˆç²¾åº¦è®­ç»ƒ
- ğŸ“ æ”¯æŒæœ€ä½³æ¨¡å‹ä¿å­˜å’Œæ£€æŸ¥ç‚¹ç®¡ç†
- ğŸ” æ”¯æŒå®šæœŸä¿å­˜å’Œè‡ªåŠ¨æ¸…ç†æ£€æŸ¥ç‚¹

## ç›®å½•ç»“æ„

```
Zer02LLM_all/
â”œâ”€â”€ model/                # æ¨¡å‹ç›¸å…³ä»£ç 
â”‚   â”œâ”€â”€ model.py         # æ ¸å¿ƒæ¨¡å‹å®ç°
â”‚   â””â”€â”€ LLMconfig.py     # æ¨¡å‹é…ç½®ç±»
â”œâ”€â”€ tokenizer/           # tokenizer ç›¸å…³æ–‡ä»¶
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”œâ”€â”€ vocab.json
â”‚   â””â”€â”€ merges.txt
â”œâ”€â”€ datasets.py          # æ•°æ®é›†å¤„ç†
â”œâ”€â”€ pretrain_sft_lora.py # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ eval_model.py        # è¯„ä¼°è„šæœ¬
â”œâ”€â”€ requirements.txt     # ä¾èµ–åŒ…åˆ—è¡¨
â””â”€â”€ README.md           # é¡¹ç›®è¯´æ˜æ–‡æ¡£
```

## å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ–
.\venv\Scripts\activate  # Windows

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 2. æ•°æ®å‡†å¤‡

ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶/ç›®å½•å­˜åœ¨ï¼š
- `./tokenizer/` - tokenizerç›®å½•ï¼ˆåŒ…å«å¿…è¦çš„tokenizeræ–‡ä»¶ï¼‰
- `./dataset/pretrain_hq.jsonl` - é¢„è®­ç»ƒæ•°æ®æ–‡ä»¶
- `./out/` - æ¨¡å‹è¾“å‡ºç›®å½•ï¼ˆä¼šè‡ªåŠ¨åˆ›å»ºï¼‰

### 3. æ¨¡å‹è®­ç»ƒ

```bash
# é¢„è®­ç»ƒæ¨¡å¼ (Pretrain)

## CPUè®­ç»ƒï¼ˆæµ‹è¯•ç”¨ï¼‰
python pretrain_sft_lora.py \
    --mode pretrain \
    --device cpu \
    --batch_size 2 \
    --epochs 1 \
    --dim 128 \
    --n_layers 2 \
    --max_seq_len 128 \
    --n_heads 4

## GPUè®­ç»ƒï¼ˆå•å¡ï¼‰- åŸºç¡€é…ç½®
python pretrain_sft_lora.py \
    --mode pretrain \
    --batch_size 32 \
    --epochs 1 \
    --learning_rate 5e-4 \
    --dim 512 \
    --n_layers 8 \
    --accumulation_steps 8 \
    --save_interval 1000 \
    --save_best_only \
    --save_last \
    --save_interval_steps 1000 \
    --keep_checkpoint_max 5

## GPUè®­ç»ƒï¼ˆå•å¡ï¼‰- å¯ç”¨Wandbç›‘æ§
python pretrain_sft_lora.py \
    --mode pretrain \
    --batch_size 32 \
    --epochs 1 \
    --learning_rate 5e-4 \
    --dim 512 \
    --n_layers 8 \
    --accumulation_steps 8 \
    --use_wandb \
    --wandb_project "Zer02LLM" \
    --wandb_log_model \
    --wandb_log_code \
    --wandb_watch_model \
    --save_best_only \
    --save_last

## GPUè®­ç»ƒï¼ˆå¤šå¡ï¼‰
torchrun --nproc_per_node=2 pretrain_sft_lora.py \
    --mode pretrain \
    --ddp \
    --batch_size 16 \
    --accumulation_steps 8 \
    --save_interval 1000 \
    --save_best_only \
    --save_last

# ç›‘ç£å¾®è°ƒæ¨¡å¼ (SFT)

## å•å¡è®­ç»ƒ
python pretrain_sft_lora.py \
    --mode sft \
    --batch_size 16 \
    --epochs 1 \
    --learning_rate 5e-6 \
    --dim 512 \
    --n_layers 8 \
    --max_seq_len 512 \
    --accumulation_steps 8 \
    --save_interval 1000 \
    --save_best_only \
    --save_last

## å¤šå¡è®­ç»ƒ
torchrun --nproc_per_node=2 pretrain_sft_lora.py \
    --mode sft \
    --ddp \
    --batch_size 8 \
    --epochs 1 \
    --learning_rate 5e-6 \
    --save_best_only \
    --save_last

## å¯ç”¨MoEçš„SFTè®­ç»ƒ
python pretrain_sft_lora.py \
    --mode sft \
    --use_moe \
    --n_routed_experts 8 \
    --num_experts_per_tok 2 \
    --batch_size 8 \
    --save_best_only \
    --save_last

# äººç±»åå¥½å¯¹é½æ¨¡å¼ (DPO)

## å•å¡è®­ç»ƒ
python pretrain_sft_lora.py \
    --mode dpo \
    --batch_size 8 \
    --epochs 2 \
    --learning_rate 1e-8 \
    --max_seq_len 3000 \
    --accumulation_steps 8 \
    --save_interval 1000 \
    --save_best_only \
    --save_last

## å¤šå¡è®­ç»ƒ
torchrun --nproc_per_node=2 pretrain_sft_lora.py \
    --mode dpo \
    --ddp \
    --batch_size 8 \
    --epochs 2 \
    --learning_rate 1e-8 \
    --max_seq_len 3000 \
    --save_best_only \
    --save_last

## å¯ç”¨MoEçš„DPOè®­ç»ƒ
python pretrain_sft_lora.py \
    --mode dpo \
    --use_moe \
    --batch_size 8 \
    --epochs 2 \
    --learning_rate 1e-8 \
    --max_seq_len 3000 \
    --save_best_only \
    --save_last

## æ˜¾å­˜ä¼˜åŒ–é…ç½®çš„DPOè®­ç»ƒ
python pretrain_sft_lora.py \
    --mode dpo \
    --batch_size 4 \
    --accumulation_steps 4 \
    --learning_rate 1e-8 \
    --max_seq_len 3000 \
    --save_best_only \
    --save_last
```

### 4. æ¨¡å‹è¯„ä¼°

```bash
# è¯„ä¼°é¢„è®­ç»ƒæ¨¡å‹
python eval_model.py --model_mode 0 --dim 512 --n_layers 8

# è¯„ä¼°SFTæ¨¡å‹ï¼ˆå¸¦æµå¼è¾“å‡ºï¼‰
python eval_model.py --model_mode 1 --dim 512 --n_layers 8 --stream True
```

## æ ¸å¿ƒåŠŸèƒ½è¯´æ˜

### 1. è®­ç»ƒæ¨¡å¼
- **é¢„è®­ç»ƒ (Pretrain)**: ä»å¤´è®­ç»ƒæ¨¡å‹
- **ç›‘ç£å¾®è°ƒ (SFT)**: åŸºäºé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¯¹è¯èƒ½åŠ›è®­ç»ƒ
- **äººç±»åå¥½å¯¹é½ (DPO)**: åŸºäºäººç±»åå¥½æ•°æ®è¿›è¡Œæ¨¡å‹å¯¹é½è®­ç»ƒ

### 2. æ¨¡å‹ç‰¹æ€§
- **æ³¨æ„åŠ›æœºåˆ¶**: æ”¯æŒæ ‡å‡†æ³¨æ„åŠ›å’Œ Flash Attention
- **ä½ç½®ç¼–ç **: ä½¿ç”¨ RoPE è¿›è¡Œä½ç½®ä¿¡æ¯ç¼–ç 
- **MoEç»“æ„**: æ”¯æŒåŠ¨æ€è·¯ç”±å’Œä¸“å®¶é€‰æ‹©
- **æ··åˆç²¾åº¦**: æ”¯æŒ FP16/BF16 è®­ç»ƒ

### 3. ä¼˜åŒ–ç‰¹æ€§
- **æ¢¯åº¦ç´¯ç§¯**: æ”¯æŒå¤§æ‰¹é‡è®­ç»ƒ
- **åˆ†å¸ƒå¼è®­ç»ƒ**: æ”¯æŒå¤šGPUè®­ç»ƒ
- **æ€§èƒ½ç›‘æ§**: æ”¯æŒ Wandb å®éªŒè¿½è¸ª
  - è¶…å‚æ•°è®°å½•ï¼šå­¦ä¹ ç‡ã€ä¼˜åŒ–å™¨é…ç½®ã€æ‰¹å¤§å°ç­‰
  - è®­ç»ƒæŒ‡æ ‡ï¼šæŸå¤±ã€æ¢¯åº¦èŒƒæ•°ã€å›°æƒ‘åº¦(perplexity)
  - GPUèµ„æºç›‘æ§ï¼šå†…å­˜å ç”¨ã€GPUåˆ©ç”¨ç‡
  - è®­ç»ƒé€Ÿåº¦ï¼šæ¯ç§’å¤„ç†tokenæ•°ã€è®­ç»ƒæ—¶é—´ä¼°è®¡
  - æ¨¡å‹æƒé‡ï¼šè‡ªåŠ¨è®°å½•æ£€æŸ¥ç‚¹æ–‡ä»¶
  - ä»£ç ä¸é…ç½®ï¼šå¯é€‰è®°å½•ä»£ç æ–‡ä»¶å’Œå®Œæ•´é…ç½®
- **æµå¼ç”Ÿæˆ**: æ”¯æŒæµå¼æ–‡æœ¬ç”Ÿæˆ
- **æ£€æŸ¥ç‚¹ç®¡ç†**: æ”¯æŒæœ€ä½³æ¨¡å‹ä¿å­˜å’Œè‡ªåŠ¨æ¸…ç†

### 4. æ£€æŸ¥ç‚¹ä¿å­˜ç­–ç•¥
- **å®šæœŸä¿å­˜**: æŒ‰æ­¥æ•°é—´éš”ä¿å­˜æ¨¡å‹
- **æœ€ä½³æ¨¡å‹**: ä¿å­˜è®­ç»ƒè¿‡ç¨‹ä¸­lossæœ€ä½çš„æ¨¡å‹
- **æœ€ç»ˆæ¨¡å‹**: ä¿å­˜è®­ç»ƒç»“æŸæ—¶çš„æ¨¡å‹çŠ¶æ€
- **è‡ªåŠ¨æ¸…ç†**: è‡ªåŠ¨åˆ é™¤æ—§çš„æ£€æŸ¥ç‚¹ä»¥èŠ‚çœç©ºé—´
- **æ•°é‡æ§åˆ¶**: å¯é…ç½®ä¿ç•™çš„æœ€å¤§æ£€æŸ¥ç‚¹æ•°é‡

### 5. Wandbç›‘æ§
- ä½¿ç”¨`--use_wandb`å¯ç”¨Wandbç›‘æ§
- ä½¿ç”¨`--wandb_log_model`è®°å½•æ¨¡å‹æƒé‡
- ä½¿ç”¨`--wandb_log_code`è®°å½•ä»£ç æ–‡ä»¶
- è°ƒæ•´`--wandb_log_freq`æ§åˆ¶è®°å½•é¢‘ç‡
- ä½¿ç”¨`--wandb_watch_model`ç›‘æ§æ¨¡å‹æ¢¯åº¦

### 6. è¶…å‚æ•°æœç´¢
- ä½¿ç”¨Weights & Biases Sweepsè¿›è¡Œè‡ªåŠ¨åŒ–è¶…å‚æ•°ä¼˜åŒ–
- æ”¯æŒè´å¶æ–¯ä¼˜åŒ–ã€ç½‘æ ¼æœç´¢å’Œéšæœºæœç´¢
- è‡ªåŠ¨æ—©åœæœºåˆ¶ï¼ŒèŠ‚çœè®¡ç®—èµ„æº
- å¯è§†åŒ–è¶…å‚æ•°é‡è¦æ€§å’Œç›¸å…³æ€§
- æ”¯æŒä¸åŒè®­ç»ƒæ¨¡å¼çš„ä¸“ç”¨é…ç½®

```bash
# å¯åŠ¨é¢„è®­ç»ƒæ¨¡å¼çš„è¶…å‚æ•°æœç´¢ï¼ˆè¿è¡Œ10æ¬¡å®éªŒï¼‰
python run_sweep.py --config sweep_config.yaml --count 10 --mode pretrain

# å¯åŠ¨DPOæ¨¡å¼çš„è¶…å‚æ•°æœç´¢ï¼ˆè¿è¡Œ5æ¬¡å®éªŒï¼‰
python run_sweep.py --config sweep_config_dpo.yaml --count 5 --mode dpo

# åœ¨ç‰¹å®šGPUä¸Šè¿è¡Œè¶…å‚æ•°æœç´¢
python run_sweep.py --config sweep_config.yaml --count 3 --gpu 0,1
```

## å‚æ•°è¯´æ˜

### æ¨¡å‹å‚æ•°
| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| dim | éšè—å±‚ç»´åº¦ | 512 |
| n_layers | å±‚æ•° | 8 |
| n_heads | æ³¨æ„åŠ›å¤´æ•° | 8 |
| max_seq_len | æœ€å¤§åºåˆ—é•¿åº¦ | 2048 |
| use_moe | æ˜¯å¦å¯ç”¨ MoE | False |

### è®­ç»ƒå‚æ•°
| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| learning_rate | å­¦ä¹ ç‡ | 5e-4 |
| batch_size | æ‰¹æ¬¡å¤§å° | 32 |
| accumulation_steps | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° | 8 |
| epochs | è®­ç»ƒè½®æ•° | 1 |

### DPOè®­ç»ƒå‚æ•°
| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| learning_rate | DPOå­¦ä¹ ç‡ | 1e-8 |
| batch_size | æ‰¹æ¬¡å¤§å° | 8 |
| max_seq_len | åºåˆ—é•¿åº¦ | 3000 |
| epochs | è®­ç»ƒè½®æ•° | 2 |

### æ£€æŸ¥ç‚¹å‚æ•°
| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| save_best_only | æ˜¯å¦åªä¿å­˜æœ€ä½³æ¨¡å‹ | True |
| save_last | æ˜¯å¦ä¿å­˜æœ€åä¸€ä¸ªepochçš„æ¨¡å‹ | True |
| save_interval_steps | æ¯å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹ | 1000 |
| keep_checkpoint_max | æœ€å¤šä¿å­˜å¤šå°‘ä¸ªæ£€æŸ¥ç‚¹ | 5 |

## å¼€æºåè®®

æœ¬é¡¹ç›®é‡‡ç”¨ MIT åè®® - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

## å®Œæ•´å‚æ•°é…ç½®è¯´æ˜

### åŸºç¡€è®­ç»ƒå‚æ•°
| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ | å¯é€‰å€¼ |
|------|------|--------|--------|
| mode | è®­ç»ƒæ¨¡å¼ | pretrain | pretrain/sft/dpo/test |
| out_dir | è¾“å‡ºç›®å½• | out | ä»»æ„æœ‰æ•ˆè·¯å¾„ |
| epochs | è®­ç»ƒè½®æ•° | 1 | æ­£æ•´æ•° |
| batch_size | æ‰¹æ¬¡å¤§å° | 32 | æ­£æ•´æ•° |
| learning_rate | å­¦ä¹ ç‡ | 5e-4 | æµ®ç‚¹æ•° |
| device | è®­ç»ƒè®¾å¤‡ | cuda:0/cpu | cuda:N/cpu |
| dtype | æ•°æ®ç±»å‹ | bfloat16 | float16/bfloat16/float32 |

### æ—¥å¿—å’Œç›‘æ§å‚æ•°
| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ | å¯é€‰å€¼ |
|------|------|--------|--------|
| use_wandb | æ˜¯å¦ä½¿ç”¨wandb | False | True/False |
| wandb_project | wandbé¡¹ç›®å | Zer02LLM | å­—ç¬¦ä¸² |
| wandb_run_name | wandbè¿è¡Œåç§° | None | å­—ç¬¦ä¸²/None |
| wandb_log_model | æ˜¯å¦è®°å½•æ¨¡å‹æƒé‡ | False | True/False |
| wandb_log_code | æ˜¯å¦è®°å½•ä»£ç  | False | True/False |
| wandb_log_freq | wandbè®°å½•é¢‘ç‡ | 1 | æ­£æ•´æ•° |
| wandb_watch_model | æ˜¯å¦ç›‘æ§æ¨¡å‹æ¢¯åº¦ | False | True/False |
| wandb_watch_log | wandb.watchçš„logå‚æ•° | gradients | gradients/all/None |
| wandb_watch_log_freq | wandb.watchçš„log_freq | 100 | æ­£æ•´æ•° |
| log_interval | æ—¥å¿—è®°å½•é—´éš” | 100 | æ­£æ•´æ•° |
| save_interval | ä¿å­˜é—´éš” | 100 | æ­£æ•´æ•° |

### æ£€æŸ¥ç‚¹ä¿å­˜å‚æ•°
| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ | å¯é€‰å€¼ |
|------|------|--------|--------|
| save_best_only | æ˜¯å¦åªä¿å­˜æœ€ä½³æ¨¡å‹ | True | True/False |
| save_last | æ˜¯å¦ä¿å­˜æœ€åä¸€ä¸ªepochçš„æ¨¡å‹ | True | True/False |
| save_interval_steps | æ¯å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹ | 1000 | æ­£æ•´æ•° |
| keep_checkpoint_max | æœ€å¤šä¿å­˜å¤šå°‘ä¸ªæ£€æŸ¥ç‚¹ | 5 | æ­£æ•´æ•° |

### åˆ†å¸ƒå¼è®­ç»ƒå‚æ•°
| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ | å¯é€‰å€¼ |
|------|------|--------|--------|
| num_workers | æ•°æ®åŠ è½½çº¿ç¨‹æ•° | 1 | æ­£æ•´æ•° |
| ddp | æ˜¯å¦ä½¿ç”¨DDP | False | True/False |
| local_rank | æœ¬åœ°è¿›ç¨‹åºå· | -1 | æ•´æ•° |

### ä¼˜åŒ–å™¨å‚æ•°
| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ | å¯é€‰å€¼ |
|------|------|--------|--------|
| accumulation_steps | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° | 8 | æ­£æ•´æ•° |
| grad_clip | æ¢¯åº¦è£å‰ªå€¼ | 1.0 | æ­£æµ®ç‚¹æ•° |
| warmup_iters | é¢„çƒ­è¿­ä»£æ¬¡æ•° | 0 | éè´Ÿæ•´æ•° |

### æ¨¡å‹ç»“æ„å‚æ•°
| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ | å¯é€‰å€¼ |
|------|------|--------|--------|
| dim | éšè—å±‚ç»´åº¦ | 512 | æ­£æ•´æ•° |
| n_layers | å±‚æ•° | 8 | æ­£æ•´æ•° |
| max_seq_len | æœ€å¤§åºåˆ—é•¿åº¦ | 512 | æ­£æ•´æ•° |
| vocab_size | è¯è¡¨å¤§å° | 4000 | æ­£æ•´æ•° |
| n_heads | æ³¨æ„åŠ›å¤´æ•° | 8 | æ­£æ•´æ•° |
| n_kv_heads | KVæ³¨æ„åŠ›å¤´æ•° | None | æ­£æ•´æ•°/None |
| rope_theta | RoPEè§’åº¦å‚æ•° | 10000.0 | æ­£æµ®ç‚¹æ•° |
| dropout | Dropoutæ¯”ç‡ | 0.1 | 0~1æµ®ç‚¹æ•° |
| norm_eps | å½’ä¸€åŒ–epsilon | 1e-8 | æ­£æµ®ç‚¹æ•° |
| multiple_of | ç»´åº¦å€æ•° | 64 | æ­£æ•´æ•° |
| flash_attn | æ˜¯å¦ä½¿ç”¨Flash Attention | False | True/False |

### MoEç›¸å…³å‚æ•°
| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ | å¯é€‰å€¼ |
|------|------|--------|--------|
| use_moe | æ˜¯å¦å¯ç”¨MoE | False | True/False |
| n_routed_experts | è·¯ç”±ä¸“å®¶æ•°é‡ | 8 | æ­£æ•´æ•° |
| num_experts_per_tok | æ¯ä¸ªtokençš„ä¸“å®¶æ•° | 2 | æ­£æ•´æ•° |
| n_shared_experts | å…±äº«ä¸“å®¶æ•°é‡ | 0 | éè´Ÿæ•´æ•° |
| scoring_func | è¯„åˆ†å‡½æ•° | softmax | softmax |
| aux_loss_alpha | è¾…åŠ©æŸå¤±æƒé‡ | 0.1 | æ­£æµ®ç‚¹æ•° |
| seq_aux | æ˜¯å¦ä½¿ç”¨åºåˆ—è¾…åŠ©æŸå¤± | True | True/False |
| norm_topk_prob | æ˜¯å¦å½’ä¸€åŒ–topkæ¦‚ç‡ | True | True/False |

### DPOç›¸å…³å‚æ•°
| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ | å¯é€‰å€¼ |
|------|------|--------|--------|
| dpo_beta | DPO lossçš„betaå‚æ•° | 0.1 | æ­£æµ®ç‚¹æ•° |
| ref_model_path | å‚è€ƒæ¨¡å‹è·¯å¾„ | None | å­—ç¬¦ä¸²/None |

### æ•°æ®ç›¸å…³å‚æ•°
| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ | å¯é€‰å€¼ |
|------|------|--------|--------|
| data_path | è®­ç»ƒæ•°æ®è·¯å¾„ | ./dataset/pretrain_hq.jsonl | æœ‰æ•ˆæ–‡ä»¶è·¯å¾„ |

## æ˜¾å­˜ä¼˜åŒ–é…ç½®

```bash
# 16GBæ˜¾å­˜é…ç½®
python pretrain_sft_lora.py --mode pretrain --dim 512 --n_layers 8 --batch_size 16 --accumulation_steps 16

# 24GBæ˜¾å­˜é…ç½®
python pretrain_sft_lora.py --mode pretrain --dim 768 --n_layers 12 --batch_size 24 --accumulation_steps 8

# 40GBæ˜¾å­˜é…ç½®
python pretrain_sft_lora.py --mode pretrain --dim 1024 --n_layers 16 --batch_size 32 --accumulation_steps 4

# DPOè®­ç»ƒ 16GBæ˜¾å­˜é…ç½®
python pretrain_sft_lora.py --mode dpo --dim 512 --n_layers 8 --batch_size 4 --accumulation_steps 8 --max_seq_len 2048

# DPOè®­ç»ƒ 24GBæ˜¾å­˜é…ç½®
python pretrain_sft_lora.py --mode dpo --dim 768 --n_layers 12 --batch_size 6 --accumulation_steps 6 --max_seq_len 2048

# DPOè®­ç»ƒ 40GBæ˜¾å­˜é…ç½®
python pretrain_sft_lora.py --mode dpo --dim 1024 --n_layers 16 --batch_size 8 --accumulation_steps 4 --max_seq_len 3000
```

## å¸¸è§é—®é¢˜è§£å†³

### 1. æ˜¾å­˜ä¸è¶³
- å‡å° batch_size
- å¢åŠ  accumulation_steps
- å‡å°æ¨¡å‹ç»´åº¦æˆ–å±‚æ•°
- ä½¿ç”¨ float16/bfloat16 ç²¾åº¦

### 2. è®­ç»ƒé€Ÿåº¦ä¼˜åŒ–
- å¯ç”¨ Flash Attention
- ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
- ä¼˜åŒ–æ•°æ®åŠ è½½ï¼ˆå¢åŠ  num_workersï¼‰
- ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ

### 3. è®­ç»ƒç¨³å®šæ€§
- è°ƒæ•´å­¦ä¹ ç‡
- ä½¿ç”¨ warmup
- å¯ç”¨æ¢¯åº¦è£å‰ª
- è°ƒæ•´ batch_size å’Œ accumulation_steps

### 4. æ£€æŸ¥ç‚¹ç®¡ç†
- åˆç†è®¾ç½®ä¿å­˜é—´éš”
- æ§åˆ¶æ£€æŸ¥ç‚¹æ•°é‡
- åŠæ—¶æ¸…ç†æ—§æ£€æŸ¥ç‚¹
- ä¿å­˜æœ€ä½³æ¨¡å‹

### 5. Wandbç›‘æ§
- ä½¿ç”¨`--use_wandb`å¯ç”¨Wandbç›‘æ§
- ä½¿ç”¨`--wandb_log_model`è®°å½•æ¨¡å‹æƒé‡
- ä½¿ç”¨`--wandb_log_code`è®°å½•ä»£ç æ–‡ä»¶
- è°ƒæ•´`--wandb_log_freq`æ§åˆ¶è®°å½•é¢‘ç‡
- ä½¿ç”¨`--wandb_watch_model`ç›‘æ§æ¨¡å‹æ¢¯åº¦

### 6. è¶…å‚æ•°æœç´¢
- ä½¿ç”¨Weights & Biases Sweepsè¿›è¡Œè‡ªåŠ¨åŒ–è¶…å‚æ•°ä¼˜åŒ–
- æ”¯æŒè´å¶æ–¯ä¼˜åŒ–ã€ç½‘æ ¼æœç´¢å’Œéšæœºæœç´¢
- è‡ªåŠ¨æ—©åœæœºåˆ¶ï¼ŒèŠ‚çœè®¡ç®—èµ„æº
- å¯è§†åŒ–è¶…å‚æ•°é‡è¦æ€§å’Œç›¸å…³æ€§
- æ”¯æŒä¸åŒè®­ç»ƒæ¨¡å¼çš„ä¸“ç”¨é…ç½®

```bash
# å¯åŠ¨é¢„è®­ç»ƒæ¨¡å¼çš„è¶…å‚æ•°æœç´¢ï¼ˆè¿è¡Œ10æ¬¡å®éªŒï¼‰
python run_sweep.py --config sweep_config.yaml --count 10 --mode pretrain

# å¯åŠ¨DPOæ¨¡å¼çš„è¶…å‚æ•°æœç´¢ï¼ˆè¿è¡Œ5æ¬¡å®éªŒï¼‰
python run_sweep.py --config sweep_config_dpo.yaml --count 5 --mode dpo

# åœ¨ç‰¹å®šGPUä¸Šè¿è¡Œè¶…å‚æ•°æœç´¢
python run_sweep.py --config sweep_config.yaml --count 3 --gpu 0,1
```

