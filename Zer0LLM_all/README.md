# Zer02LLM

ä¸€ä¸ªè½»é‡çº§çš„å¤§è¯­è¨€æ¨¡å‹(LLM)è®­ç»ƒå’Œæ¨ç†æ¡†æ¶ï¼Œä¸“æ³¨äºé«˜æ•ˆå®ç°å’Œæ€§èƒ½ä¼˜åŒ–ã€‚

## é¡¹ç›®ç‰¹ç‚¹

- ğŸš€ é«˜æ•ˆçš„ Transformer æ¶æ„å®ç°
- ğŸ¯ æ”¯æŒ MoE (Mixture of Experts) ç»“æ„
- ğŸ’¡ å®ç°äº† Flash Attention ä¼˜åŒ–
- ğŸ”„ æ”¯æŒ RoPE (Rotary Position Embedding) ä½ç½®ç¼–ç 
- ğŸ“¦ æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ‰©å±•
- ğŸ›  å†…ç½®æ€§èƒ½ä¼˜åŒ–æœºåˆ¶
- ğŸ”¤ æ”¯æŒè‡ªå®šä¹‰ tokenizer
- ğŸ“Š æ”¯æŒ Wandb å®éªŒè¿½è¸ª
- ğŸ”„ æ”¯æŒæµå¼æ¨ç†è¾“å‡º
- ğŸ’¾ æ”¯æŒæ¢¯åº¦ç´¯ç§¯å’Œæ··åˆç²¾åº¦è®­ç»ƒ

## ç›®å½•ç»“æ„

```
Zer02LLM_all/
â”œâ”€â”€ model/                # æ¨¡å‹ç›¸å…³ä»£ç 
â”‚   â”œâ”€â”€ model.py         # æ ¸å¿ƒæ¨¡å‹å®ç°
â”‚   â””â”€â”€ LLMconfig.py     # æ¨¡å‹é…ç½®ç±»
â”œâ”€â”€ tokenizer/           # tokenizer ç›¸å…³æ–‡ä»¶
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”œâ”€â”€ vocab.json
â”‚   â””â”€â”€ merges.txt
â”œâ”€â”€ datasets.py          # æ•°æ®é›†å¤„ç†
â”œâ”€â”€ pretrain_sft_lora.py # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ eval_model.py        # è¯„ä¼°è„šæœ¬
â”œâ”€â”€ requirements.txt     # ä¾èµ–åŒ…åˆ—è¡¨
â””â”€â”€ README.md           # é¡¹ç›®è¯´æ˜æ–‡æ¡£
```

## å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ–
.\venv\Scripts\activate  # Windows

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 2. æ•°æ®å‡†å¤‡

ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶/ç›®å½•å­˜åœ¨ï¼š
- `./tokenizer/` - tokenizerç›®å½•ï¼ˆåŒ…å«å¿…è¦çš„tokenizeræ–‡ä»¶ï¼‰
- `./dataset/pretrain_hq.jsonl` - é¢„è®­ç»ƒæ•°æ®æ–‡ä»¶
- `./out/` - æ¨¡å‹è¾“å‡ºç›®å½•ï¼ˆä¼šè‡ªåŠ¨åˆ›å»ºï¼‰

### 3. æ¨¡å‹è®­ç»ƒ

```bash
# CPUè®­ç»ƒï¼ˆæµ‹è¯•ç”¨ï¼‰
python pretrain_sft_lora.py --device cpu --mode pretrain --batch_size 2 --epochs 1 --dim 128 --n_layers 2 --max_seq_len 128 --n_heads 4

# GPUè®­ç»ƒï¼ˆå•å¡ï¼‰
python pretrain_sft_lora.py --mode pretrain --batch_size 32 --epochs 1 --learning_rate 5e-4 --dim 512 --n_layers 8

# åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¤šå¡ï¼‰
torchrun --nproc_per_node=2 pretrain_sft_lora.py --mode pretrain --ddp --batch_size 16
```

### 4. æ¨¡å‹è¯„ä¼°

```bash
# è¯„ä¼°é¢„è®­ç»ƒæ¨¡å‹
python eval_model.py --model_mode 0 --dim 512 --n_layers 8

# è¯„ä¼°SFTæ¨¡å‹ï¼ˆå¸¦æµå¼è¾“å‡ºï¼‰
python eval_model.py --model_mode 1 --dim 512 --n_layers 8 --stream True
```

## æ ¸å¿ƒåŠŸèƒ½è¯´æ˜

### 1. è®­ç»ƒæ¨¡å¼
- **é¢„è®­ç»ƒ (Pretrain)**: ä»å¤´è®­ç»ƒæ¨¡å‹
- **ç›‘ç£å¾®è°ƒ (SFT)**: åŸºäºé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¯¹è¯èƒ½åŠ›è®­ç»ƒ

### 2. æ¨¡å‹ç‰¹æ€§
- **æ³¨æ„åŠ›æœºåˆ¶**: æ”¯æŒæ ‡å‡†æ³¨æ„åŠ›å’Œ Flash Attention
- **ä½ç½®ç¼–ç **: ä½¿ç”¨ RoPE è¿›è¡Œä½ç½®ä¿¡æ¯ç¼–ç 
- **MoEç»“æ„**: æ”¯æŒåŠ¨æ€è·¯ç”±å’Œä¸“å®¶é€‰æ‹©
- **æ··åˆç²¾åº¦**: æ”¯æŒ FP16/BF16 è®­ç»ƒ

### 3. ä¼˜åŒ–ç‰¹æ€§
- **æ¢¯åº¦ç´¯ç§¯**: æ”¯æŒå¤§æ‰¹é‡è®­ç»ƒ
- **åˆ†å¸ƒå¼è®­ç»ƒ**: æ”¯æŒå¤šGPUè®­ç»ƒ
- **æ€§èƒ½ç›‘æ§**: æ”¯æŒ Wandb å®éªŒè¿½è¸ª
- **æµå¼ç”Ÿæˆ**: æ”¯æŒæµå¼æ–‡æœ¬ç”Ÿæˆ

## æ˜¾å­˜ä¼˜åŒ–é…ç½®

```bash
# 16GBæ˜¾å­˜é…ç½®
python pretrain_sft_lora.py --mode pretrain --dim 512 --n_layers 8 --batch_size 16 --accumulation_steps 16

# 24GBæ˜¾å­˜é…ç½®
python pretrain_sft_lora.py --mode pretrain --dim 768 --n_layers 12 --batch_size 24 --accumulation_steps 8

# 40GBæ˜¾å­˜é…ç½®
python pretrain_sft_lora.py --mode pretrain --dim 1024 --n_layers 16 --batch_size 32 --accumulation_steps 4
```

## å¸¸è§é—®é¢˜è§£å†³

### 1. æ˜¾å­˜ä¸è¶³
- å‡å° batch_size
- å¢åŠ  accumulation_steps
- å‡å°æ¨¡å‹ç»´åº¦æˆ–å±‚æ•°
- ä½¿ç”¨ float16/bfloat16 ç²¾åº¦

### 2. è®­ç»ƒé€Ÿåº¦ä¼˜åŒ–
- å¯ç”¨ Flash Attention
- ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
- ä¼˜åŒ–æ•°æ®åŠ è½½ï¼ˆå¢åŠ  num_workersï¼‰
- ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ

### 3. è®­ç»ƒç¨³å®šæ€§
- è°ƒæ•´å­¦ä¹ ç‡
- ä½¿ç”¨ warmup
- å¯ç”¨æ¢¯åº¦è£å‰ª
- è°ƒæ•´ batch_size å’Œ accumulation_steps

## å‚æ•°è¯´æ˜

### æ¨¡å‹å‚æ•°
| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| dim | éšè—å±‚ç»´åº¦ | 512 |
| n_layers | å±‚æ•° | 8 |
| n_heads | æ³¨æ„åŠ›å¤´æ•° | 8 |
| max_seq_len | æœ€å¤§åºåˆ—é•¿åº¦ | 2048 |

### è®­ç»ƒå‚æ•°
| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| learning_rate | å­¦ä¹ ç‡ | 5e-4 |
| batch_size | æ‰¹æ¬¡å¤§å° | 32 |
| accumulation_steps | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° | 8 |
| epochs | è®­ç»ƒè½®æ•° | 1 |

## å¼€æºåè®®

æœ¬é¡¹ç›®é‡‡ç”¨ MIT åè®® - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

