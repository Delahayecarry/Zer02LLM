# Zer02LLM_all

ä¸€ä¸ªè½»é‡çº§çš„ LLM (Large Language Model) å®ç°é¡¹ç›®ï¼Œä¸“æ³¨äºæ¨¡å‹è®­ç»ƒå’Œæ¨ç†ä¼˜åŒ–ã€‚

## é¡¹ç›®ç‰¹ç‚¹

- ğŸš€ é«˜æ•ˆçš„ Transformer æ¶æ„å®ç°
- ğŸ¯ æ”¯æŒ MoE (Mixture of Experts) ç»“æ„
- ğŸ’¡ å®ç°äº† Flash Attention ä¼˜åŒ–
- ğŸ”„ æ”¯æŒ RoPE (Rotary Position Embedding) ä½ç½®ç¼–ç 
- ğŸ“¦ æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ‰©å±•
- ğŸ›  å†…ç½®æ€§èƒ½ä¼˜åŒ–æœºåˆ¶

## æ ¸å¿ƒåŠŸèƒ½

- **é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶**: å®ç°äº†åŒ…å« Flash Attention åœ¨å†…çš„ä¼˜åŒ–æ³¨æ„åŠ›è®¡ç®—
- **MoE ä¸“å®¶ç³»ç»Ÿ**: æ”¯æŒåŠ¨æ€è·¯ç”±å’Œä¸“å®¶é€‰æ‹©
- **ä½ç½®ç¼–ç ä¼˜åŒ–**: é‡‡ç”¨ RoPE è¿›è¡Œä½ç½®ä¿¡æ¯ç¼–ç 
- **çµæ´»çš„æ¨¡å‹é…ç½®**: æ”¯æŒè‡ªå®šä¹‰æ¨¡å‹å‚æ•°å’Œç»“æ„
- **æµå¼ç”Ÿæˆ**: æ”¯æŒæ–‡æœ¬çš„æµå¼ç”Ÿæˆè¾“å‡º

## å®‰è£…è¯´æ˜

```bash

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

## å¿«é€Ÿå¼€å§‹

```python
from model.model import LLM
from model.LLMconfig import LLMconfig

# é…ç½®æ¨¡å‹å‚æ•°
config = LLMconfig(
    vocab_size=32000,
    dim=512,
    n_layers=6,
    n_heads=8,
    max_seq_len=2048
)

# åˆå§‹åŒ–æ¨¡å‹
model = LLM(config)

# ä½¿ç”¨æ¨¡å‹è¿›è¡Œç”Ÿæˆ
output = model.generate(
    input_ids,
    max_new_tokens=100,
    temperature=0.7,
    top_p=0.9
)
```

## ä¸»è¦ç»„ä»¶

- **FFN (Feed Forward Network)**: å®ç°äº†åŸºäº SwiGLU çš„å‰é¦ˆç½‘ç»œ
- **Attention**: æ”¯æŒå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å’Œ KV ç¼“å­˜
- **MoEgate**: å®ç°äº†ä¸“å®¶é€‰æ‹©å’Œè·¯ç”±æœºåˆ¶
- **RMSnorm**: é«˜æ•ˆçš„å±‚å½’ä¸€åŒ–å®ç°

## æ€§èƒ½ä¼˜åŒ–

- å®ç°äº† Flash Attention æœºåˆ¶
- æ”¯æŒ KV ç¼“å­˜ä¼˜åŒ–æ¨ç†é€Ÿåº¦
- MoE ç»“æ„æå‡æ¨¡å‹å®¹é‡å’Œæ•ˆç‡
- ä¼˜åŒ–çš„ä½ç½®ç¼–ç è®¡ç®—

## å¼€æºåè®®

æœ¬é¡¹ç›®é‡‡ç”¨ MIT åè®® - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

## è®­ç»ƒæŒ‡å—

### ç¯å¢ƒå‡†å¤‡

åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨æœ‰ä»¥ä¸‹å¿…è¦æ–‡ä»¶å’Œç›®å½•ï¼š

- `./model/zer02llm_tokenizer` - tokenizerç›®å½•
- `./dataset/pretrain_hq.jsonl` - é¢„è®­ç»ƒæ•°æ®æ–‡ä»¶
- `./out` - æ¨¡å‹è¾“å‡ºç›®å½•(ä¼šè‡ªåŠ¨åˆ›å»º)

### è®­ç»ƒæ¨¡å¼

é¡¹ç›®æ”¯æŒä¸¤ç§ä¸»è¦çš„è®­ç»ƒæ¨¡å¼ï¼š

1. **é¢„è®­ç»ƒæ¨¡å¼ (pretrain)**: ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹
2. **SFTæ¨¡å¼ (sft)**: åœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸Šè¿›è¡Œå¾®è°ƒ

### åŸºæœ¬è®­ç»ƒå‘½ä»¤

#### é¢„è®­ç»ƒæ¨¡å¼
```bash
python pretrain_sft_lora.py \
    --mode pretrain \
    --batch_size 32 \
    --epochs 1 \
    --learning_rate 5e-4 \
    --dim 512 \
    --n_layers 8 \
    --data_path ./dataset/pretrain_hq.jsonl
```

#### SFTå¾®è°ƒæ¨¡å¼
```bash
python pretrain_sft_lora.py \
    --mode sft \
    --batch_size 32 \
    --epochs 1 \
    --learning_rate 5e-4 \
    --dim 512 \
    --n_layers 8 \
    --data_path ./dataset/sft_data.jsonl
```

### é‡è¦å‚æ•°è¯´æ˜

- `--dim`: æ¨¡å‹ç»´åº¦ï¼Œé»˜è®¤512
- `--n_layers`: æ¨¡å‹å±‚æ•°ï¼Œé»˜è®¤8
- `--max_seq_len`: æœ€å¤§åºåˆ—é•¿åº¦ï¼Œé»˜è®¤512
- `--batch_size`: æ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤32
- `--accumulation_steps`: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œé»˜è®¤8
- `--learning_rate`: å­¦ä¹ ç‡ï¼Œé»˜è®¤5e-4

### é«˜çº§è®­ç»ƒåŠŸèƒ½

#### MoE(æ··åˆä¸“å®¶)æ¨¡å‹è®­ç»ƒ
```bash
python pretrain_sft_lora.py \
    --mode pretrain \
    --use_moe \
    --n_routed_experts 8 \
    --num_experts_per_tok 2
```

#### åˆ†å¸ƒå¼è®­ç»ƒ(DDP)
```bash
torchrun --nproc_per_node=2 pretrain_sft_lora.py \
    --mode pretrain \
    --ddp \
    --batch_size 16
```

### è®­ç»ƒç›‘æ§

- ä½¿ç”¨wandbç›‘æ§ï¼šæ·»åŠ  `--use_wandb` å‚æ•°
- æ—¥å¿—é—´éš”è®¾ç½®ï¼š`--log_interval`
- æ¨¡å‹ä¿å­˜é—´éš”ï¼š`--save_interval`

### æ¨¡å‹ä¿å­˜

- æ¨¡å‹è‡ªåŠ¨ä¿å­˜åœ¨ `out` ç›®å½•
- é¢„è®­ç»ƒæ¨¡å‹ï¼š`pretrain_{dim}.pth`
- SFTæ¨¡å‹ï¼š`sft_{dim}.pth`
- MoEæ¨¡å‹ä¼šåœ¨æ–‡ä»¶ååæ·»åŠ  `_moe` åç¼€

### è®­ç»ƒå»ºè®®

1. å»ºè®®ä»å°è§„æ¨¡å¼€å§‹æµ‹è¯•ï¼š

```bash
python pretrain_sft_lora.py \
    --mode pretrain \
    --dim 128 \
    --n_layers 4 \
    --batch_size 8 \
    --epochs 1
```

2. æ˜¾å­˜ä¼˜åŒ–å»ºè®®ï¼š
   - å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå¯ä»¥ï¼š
     - å‡å° batch_size
     - å¢åŠ  accumulation_steps
     - å‡å°æ¨¡å‹ç»´åº¦(dim)æˆ–å±‚æ•°(n_layers)

3. æ•°æ®å‡†å¤‡ï¼š
   - ç¡®ä¿æ•°æ®æ ¼å¼ç¬¦åˆ `PretrainDataset` æˆ– `SFTDataset` çš„è¦æ±‚
   - æ ¹æ®GPUæ˜¾å­˜å¤§å°è°ƒæ•´è®­ç»ƒå‚æ•°

### å¸¸è§é—®é¢˜

1. **æ˜¾å­˜ä¸è¶³**ï¼š
   - é¦–å…ˆå°è¯•å‡å° batch_size
   - å¯ä»¥é€šè¿‡å¢åŠ  accumulation_steps æ¥æ¨¡æ‹Ÿæ›´å¤§çš„ batch_size
   - æœ€åè€ƒè™‘å‡å°æ¨¡å‹è§„æ¨¡

2. **è®­ç»ƒé€Ÿåº¦æ…¢**ï¼š
   - æ£€æŸ¥æ˜¯å¦å¯ç”¨äº† Flash Attention
   - è€ƒè™‘ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
   - ä¼˜åŒ–æ•°æ®åŠ è½½æµç¨‹ï¼ˆå¢åŠ  num_workersï¼‰

3. **è®­ç»ƒä¸ç¨³å®š**ï¼š
   - æ£€æŸ¥å­¦ä¹ ç‡æ˜¯å¦åˆé€‚
   - é€‚å½“è°ƒæ•´ warmup_iters
   - è€ƒè™‘ä½¿ç”¨æ¢¯åº¦è£å‰ªï¼ˆgrad_clipï¼‰

## å¿«é€Ÿè®­ç»ƒæŒ‡å—

### 1. å•å¡è®­ç»ƒ

```bash
# åˆ›å»ºè®­ç»ƒè„šæœ¬ train.sh
cat << EOF > train.sh
#!/bin/bash

# è®¾ç½® CUDA å¯è§è®¾å¤‡
export CUDA_VISIBLE_DEVICES=0

# åŸºç¡€è®­ç»ƒå‘½ä»¤
python pretrain_sft_lora.py \
    --mode pretrain \
    --batch_size 32 \
    --epochs 1 \
    --learning_rate 5e-4 \
    --dim 512 \
    --n_layers 8 \
    --data_path ./dataset/pretrain_hq.jsonl
EOF

# æ·»åŠ æ‰§è¡Œæƒé™
chmod +x train.sh

# å¯åŠ¨è®­ç»ƒ
./train.sh
```

### 2. å¤šå¡åˆ†å¸ƒå¼è®­ç»ƒ

```bash
# åˆ›å»ºåˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬ train_ddp.sh
cat << EOF > train_ddp.sh
#!/bin/bash

# è®¾ç½® CUDA å¯è§è®¾å¤‡
export CUDA_VISIBLE_DEVICES=0,1,2,3

# è·å– GPU æ•°é‡
NUM_GPUS=\$(echo \$CUDA_VISIBLE_DEVICES | tr ',' '\n' | wc -l)

# å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ
torchrun --nproc_per_node=\$NUM_GPUS pretrain_sft_lora.py \
    --mode pretrain \
    --ddp \
    --batch_size 16 \
    --epochs 1 \
    --learning_rate 5e-4 \
    --dim 512 \
    --n_layers 8 \
    --data_path ./dataset/pretrain_hq.jsonl
EOF

# æ·»åŠ æ‰§è¡Œæƒé™
chmod +x train_ddp.sh

# å¯åŠ¨è®­ç»ƒ
./train_ddp.sh
```

### 3. åå°è®­ç»ƒï¼ˆä½¿ç”¨ tmuxï¼‰

```bash
# å®‰è£… tmuxï¼ˆå¦‚æœæœªå®‰è£…ï¼‰
apt-get install tmux  # Ubuntu/Debian
# æˆ–
yum install tmux      # CentOS/RHEL

# åˆ›å»ºæ–°çš„ tmux ä¼šè¯
tmux new -s train

# åœ¨ tmux ä¼šè¯ä¸­å¯åŠ¨è®­ç»ƒ
./train.sh  # å•å¡è®­ç»ƒ
# æˆ–
./train_ddp.sh  # å¤šå¡è®­ç»ƒ

# åˆ†ç¦» tmux ä¼šè¯ï¼ˆCtrl+B ç„¶åæŒ‰ Dï¼‰
# é‡æ–°è¿æ¥ä¼šè¯
tmux attach -t train
```

### 4. è®­ç»ƒå‚æ•°è¯´æ˜

#### åŸºç¡€å‚æ•°
- `--mode`: è®­ç»ƒæ¨¡å¼ [pretrain/sft]
- `--batch_size`: æ‰¹æ¬¡å¤§å°
- `--epochs`: è®­ç»ƒè½®æ•°
- `--learning_rate`: å­¦ä¹ ç‡
- `--dim`: æ¨¡å‹ç»´åº¦
- `--n_layers`: æ¨¡å‹å±‚æ•°

#### é«˜çº§å‚æ•°
- `--max_seq_len`: æœ€å¤§åºåˆ—é•¿åº¦ï¼Œé»˜è®¤512
- `--accumulation_steps`: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œé»˜è®¤8
- `--dtype`: æ•°æ®ç±»å‹ [float32/float16/bfloat16]
- `--flash_attn`: å¯ç”¨ Flash Attention
- `--use_wandb`: å¯ç”¨ Wandb ç›‘æ§

### 5. æ˜¾å­˜ä¼˜åŒ–å»ºè®®

æ ¹æ®æ˜¾å¡æ˜¾å­˜å¤§å°é€‰æ‹©åˆé€‚çš„é…ç½®ï¼š

#### 16GB æ˜¾å­˜é…ç½®
```bash
python pretrain_sft_lora.py \
    --mode pretrain \
    --dim 512 \
    --n_layers 8 \
    --batch_size 16 \
    --accumulation_steps 16 \
    --max_seq_len 512
```

#### 24GB æ˜¾å­˜é…ç½®
```bash
python pretrain_sft_lora.py \
    --mode pretrain \
    --dim 768 \
    --n_layers 12 \
    --batch_size 24 \
    --accumulation_steps 8 \
    --max_seq_len 512
```

#### 40GB+ æ˜¾å­˜é…ç½®
```bash
python pretrain_sft_lora.py \
    --mode pretrain \
    --dim 1024 \
    --n_layers 16 \
    --batch_size 32 \
    --accumulation_steps 4 \
    --max_seq_len 1024
```

### 6. è®­ç»ƒç›‘æ§

1. å®æ—¶æŸ¥çœ‹æ—¥å¿—ï¼š
```bash
tail -f train.log  # å¦‚æœå°†è¾“å‡ºé‡å®šå‘åˆ°äº†æ—¥å¿—æ–‡ä»¶
```

2. æŸ¥çœ‹ GPU ä½¿ç”¨æƒ…å†µï¼š
```bash
watch -n 1 nvidia-smi  # æ¯ç§’æ›´æ–°ä¸€æ¬¡
```

3. ä½¿ç”¨ Wandb ç›‘æ§ï¼ˆéœ€è¦å…ˆæ³¨å†Œè´¦å·ï¼‰ï¼š
```bash
# æ·»åŠ  wandb å‚æ•°
python pretrain_sft_lora.py \
    --mode pretrain \
    --use_wandb \
    --wandb_project "é¡¹ç›®åç§°" \
    [å…¶ä»–å‚æ•°]
```

### 7. å¸¸è§é—®é¢˜å¤„ç†

1. æ˜¾å­˜ä¸è¶³ï¼š
   - å‡å° batch_size
   - å¢åŠ  accumulation_steps
   - å‡å°æ¨¡å‹ç»´åº¦(dim)æˆ–å±‚æ•°(n_layers)
   - ä½¿ç”¨ float16/bfloat16 ç²¾åº¦

2. è®­ç»ƒä¸­æ–­æ¢å¤ï¼š
   - ä½¿ç”¨ tmux å¯ä»¥é˜²æ­¢ SSH æ–­å¼€å½±å“
   - å¯ä»¥ä»æœ€è¿‘çš„æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ

3. å¤šå¡è®­ç»ƒé—®é¢˜ï¼š
   - ç¡®ä¿ CUDA_VISIBLE_DEVICES è®¾ç½®æ­£ç¡®
   - æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼ˆå¤šæœºè®­ç»ƒæ—¶ï¼‰
   - é€‚å½“è°ƒæ•´æ¯å¼ å¡çš„ batch_size

