{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LLMConfig' from 'model.LLMconfig' (e:\\xiangmu\\GITHUB_PROJS\\Zero2LLM\\Zer02LLM\\Zer0LLM_all\\model\\LLMconfig.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLLMconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMConfig\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Optional, Tuple, List\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'LLMConfig' from 'model.LLMconfig' (e:\\xiangmu\\GITHUB_PROJS\\Zero2LLM\\Zer02LLM\\Zer0LLM_all\\model\\LLMconfig.py)"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import struct\n",
    "import inspect\n",
    "import time\n",
    "\n",
    "from model.LLMconfig import LLMConfig\n",
    "from typing import Any, Optional, Tuple, List\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from transformers import PreTrainedModel\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 均方根层归一化(Root Mean Square Layer Normalization, RMSNorm)\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.weight * x * torch.rsqrt(torch.mean(x.pow(2), dim=-1, keepdim=True) + self.eps)\n",
    "\n",
    "#2 旋转位置编码(Rotary Position Embedding, RoPE)\n",
    "def precompute_pos_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    # 计算旋转位置编码的频率\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim))\n",
    "    # 生成时间步长\n",
    "    t = torch.arange(end, dtype=freqs.dtype)  # type: ignore\n",
    "    # 计算频率的外积\n",
    "    freqs = torch.outer(t, freqs)  # type: ignore\n",
    "    pos_cis = torch.polar(torch.ones_like(freqs), freqs)  # type: ignore\n",
    "    return pos_cis\n",
    "\n",
    "def apply_rotary_emb(xq, xk, pos_cis):\n",
    "    def unite_shape(pos_cis, x):\n",
    "        ndim = x.ndim # 获取输入张量 x 的维度\n",
    "        assert 0 <= 1 < ndim # 检查 pos_cis 的维度是否正确\n",
    "        assert pos_cis.shape == (x.shape[1], x.shape[-1]) # 检查 pos_cis 的形状是否正确\n",
    "        shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "        # 将 pos_cis 和 x 的形状调整为相同的形状\n",
    "        return pos_cis.view(*shape)\n",
    "    \n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "\n",
    "    pos_cis = unite_shape(pos_cis, xq_)\n",
    "    # pos_cis shape: [batch_size, seq_len, n_embd//2]\n",
    "    # 保留实部\n",
    "    xq_out = torch.view_as_real(xq_ * pos_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * pos_cis).flatten(3)\n",
    "    # xq_out shape: [batch_size, seq_len, n_embd]\n",
    "    # xk_out shape: [batch_size, seq_len, n_embd]\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_cis 的形状为 torch.Size([16, 32]), 其中 [0, 0] 下标元素为 (1+0j)\n",
      "\n",
      "经过 RoPE 编码后的 Query 与 Key 的形状为 torch.Size([2, 16, 4, 64]),  torch.Size([2, 16, 4, 64])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xq,  xk = torch.randn((2,  16,  4,  64)), torch.randn((2,  16,  4,  64)) # (batch_size,  sequence_length,  num_heads,  head_dim)\n",
    "pos_cis = precompute_pos_cis(64,  16) # 计算旋转位置编码的旋转角（复数表示）\n",
    "print(f'pos_cis 的形状为 {pos_cis.shape}, 其中 [0, 0] 下标元素为 {pos_cis[0,  0]}\\n')\n",
    "\n",
    "xq_rope,  xk_rope = apply_rotary_emb(xq,  xk,  pos_cis)\n",
    "# original shape xq: torch.Size([2, 16, 4, 64])\n",
    "# ajusted shape xq: torch.Size([2, 16, 4, 32])\n",
    "# ajusted shape pos_cis： torch.Size([1, 16, 1, 32])\n",
    "print(f'经过 RoPE 编码后的 Query 与 Key 的形状为 {xq_rope.shape},  {xk_rope.shape}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (4084081713.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[23], line 18\u001b[1;36m\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# 3 Attention 层\n",
    "# repeatkv\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    bs, seqlen, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:, :, :, None, :]\n",
    "        .expand(bs, seqlen, n_kv_heads, n_rep, head_dim)\n",
    "        .reshape(bs, seqlen, n_kv_heads * n_rep, head_dim)\n",
    "    )# 重复 Key 和 Value 的最后一个维度 n_rep 次\n",
    "\n",
    "# attention\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,  args: LLMConfig):\n",
    "        super().__init__()\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        assert args.n_heads % self.n_kv_heads == 0\n",
    "        self.n_local_heads = args.n_heads\n",
    "        self.n_local_kv_heads = args.n_kv_heads\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        # q k v o projection\n",
    "        self.wq = nn.Linear(args.dim,  args.n_heads * self.head_dim,  bias=False)\n",
    "        self.wk = nn.Linear(args.dim,  args.n_kv_heads * self.head_dim,  bias=False)\n",
    "        self.wv = nn.Linear(args.dim,  args.n_kv_heads * self.head_dim,  bias=False)\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim,  args.dim,  bias=False)\n",
    "        self.attn_dropout = nn.Dropout(args.dropout)\n",
    "        self.resid_dropout = nn.Dropout(args.dropout)\n",
    "        self.dropout = args.dropout\n",
    "        self.flash = hasattr(torch.nn.functional,  'scaled_dot_product_attention') and args.flash_attn\n",
    "        mask = torch.full((1,  1,  args.max_seq_len,  args.max_seq_len),  float(\"-inf\"))\n",
    "        mask = torch.triu(mask,  diagonal=1)\n",
    "        self.register_buffer(\"mask\",  mask,  persistent=False)\n",
    "\n",
    "    def forward(self, \n",
    "               x: torch.Tensor, \n",
    "               pos_cis: torch.Tensor, \n",
    "               past_key_value: Optional[Tuple[torch.Tensor,  torch.Tensor]] = None, \n",
    "               use_cache=False):\n",
    "        bsz,  seq_len,  _ = x.shape\n",
    "        ############## Forward QKV & RoPE ##############\n",
    "        xq,  xk,  xv = self.wq(x),  self.wk(x),  self.wv(x)\n",
    "        xq = xq.view(bsz,  seq_len,  self.n_local_heads,  self.head_dim)\n",
    "        xk = xk.view(bsz,  seq_len,  self.n_local_kv_heads,  self.head_dim)\n",
    "        xv = xv.view(bsz,  seq_len,  self.n_local_kv_heads,  self.head_dim)\n",
    "        xq,  xv = apply_rotary_emb(xq,  xk,  pos_cis)\n",
    "        ################### KV Cache ###################\n",
    "        if past_key_value is not None:\n",
    "            xk = torch.cat([past_key_value[0],  xk],  dim=1)\n",
    "            xv = torch.cat([past_key_value[1],  xv],  dim=1)\n",
    "        past_kv = (xk,  xv) if use_cache else None\n",
    "        xq,  xk,  xv = (\n",
    "            xq.transpose(1,  2), \n",
    "            repeat_kv(xk,  self.n_rep).transpose(1,  2), \n",
    "            repeat_kv(xv,  self.n_rep).transpose(1,  2)\n",
    "        )\n",
    "        ############ Scaled Dot Production #############\n",
    "        if self.flash and seq_len != 1:\n",
    "            dropout_p = self.dropout if self.training else 0.0\n",
    "            output = F.scaled_dot_product_attention(\n",
    "                xq,  xk,  xv, \n",
    "                attn_mask=None, \n",
    "                dropout_p=dropout_p, \n",
    "                is_causal=True\n",
    "            )\n",
    "        else:\n",
    "            scores = (xq @ xk.transpose(-2,  -1)) / math.sqrt(self.head_dim)\n",
    "            scores += self.mask[:,  :,  :seq_len,  :seq_len]\n",
    "            scores = F.softmax(scores.float(),  dim=-1).type_as(xq)\n",
    "            scores = self.attn_dropout(scores)\n",
    "            output = scores @ xv\n",
    "        ################################################\n",
    "        output = output.transpose(1,  2).reshape(bsz,  seq_len,  -1)\n",
    "        output = self.resid_dropout(self.wo(output))\n",
    "        return output,  past_kv\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
